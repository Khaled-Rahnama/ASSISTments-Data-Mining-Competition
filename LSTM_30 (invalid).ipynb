{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changelog:\n",
    "\n",
    "* v8 Checking code and revising the debug files. Ready for debugging \n",
    "* V9 Adding resampling\n",
    "* V10 Adding tensorboard\n",
    "* V11 Reducing LSTM layer to 1 in order to have better interpretation in tensorboard\n",
    "* V12 adding actionId to data in order to debug datasets and step better + fixing the wrong column name for x_prepared + **fixing LABEL WRONG!!!!! ASSIENMENT **\n",
    "* V13 removing masking layer and padding with 0 or 9999\n",
    "* V14 removing two outputs\n",
    "* V15 returning the masking layer with mask value 99.\n",
    "    - The result is that without adding masking layer the training accuracy does not increase at all and it remains the same on nearly 50\n",
    "    - however the validation accuracy remains the same around 50 and does not increase while the validation loss is being increased as allways\n",
    "* V16 Changing the optimizer to SGD\n",
    "* V17 Changing the optimizer to rmsprop\n",
    "* V18 Increasing batch size from 1 to higher and coming back the optimizer to Adam (both roc and accuracy on train was about .9 while the accuracy of validation was 0.5\n",
    "* V19 Separating validation set from data and feed to the fit function using validation_data param\n",
    "* V20 Adding another layer of 100 unit\n",
    "* V21 Joining per stud features\n",
    "* V22 removing the sampling\n",
    "* V23 Add two output\n",
    "* V24 Assumes that we are overfitting so we are going to:\n",
    "    - Remove additional LSTM layer to simplify the model $\\checkmark$\n",
    "    - Reduce the number of units for LSTM layer $\\checkmark$ (saw that finally validation loss starts to deacrease with 20 unit and 200 seq lenght)\n",
    "    - Reduce the number of features as much as possible (should be done after dinormalizing the binary variables)\n",
    "    - Reduce the number of seq length $\\checkmark$\n",
    "* V25 Excluding binary variables and some other proportional variables from the standardization step (it seems we are improving!)\n",
    "* V26 Adding competition validation set for making prediction on un-labeled data\n",
    "* Bringing resampling back to the game!\n",
    "* V28 Adding Startify spliting\n",
    "* V29 Inversing the sequence! (it make more sence to feed the old frames first and then go forward for newer frames)\n",
    "* V30 A revolutionary update: removing the padding and updating the weights with variable lenght sequences!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "var kernel = IPython.notebook.kernel;\n",
       "var thename = window.document.getElementById(\"notebook_name\").innerHTML;\n",
       "var command = \"theNotebook = \" + \"'\"+thename+\"'\";\n",
       "kernel.execute(command);"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "var kernel = IPython.notebook.kernel;\n",
    "var thename = window.document.getElementById(\"notebook_name\").innerHTML;\n",
    "var command = \"theNotebook = \" + \"'\"+thename+\"'\";\n",
    "kernel.execute(command);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from Vis import plot_loss, plot_roc\n",
    "from Preprocessing import Preprocessing\n",
    "from Preprocessing import Cols\n",
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "import pandas as pd\n",
    "from keras.layers import Dropout\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "from keras import optimizers\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "# pandas.set_option('max_columns',10)\n",
    "import os\n",
    "import glob\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_files = glob.glob(os.path.join(\"Dataset\", \"student_log_*.csv\"))\n",
    "raw_dataset = pd.concat((pd.read_csv(f, index_col=[\"ITEST_id\"]) for f in data_files))\n",
    "\n",
    "dataset = raw_dataset.drop(Cols.excluded_cols + Cols.cat_cols, axis=1)\n",
    "\n",
    "labels = DataFrame.from_csv(\"Dataset/training_label.csv\")\n",
    "valid_test_label_dataset = DataFrame.from_csv(\"Dataset/validation_test_label.csv\")\n",
    "\n",
    "unlabels = valid_test_label_dataset.drop(list(labels.index.intersection(valid_test_label_dataset.index)))\n",
    "labels_unlabels = pd.concat([labels,unlabels])\n",
    "\n",
    "dwlu = dataset.join(labels_unlabels, how=\"inner\") # dwlu = dataset_with_labels_unlabels\n",
    "\n",
    "dwlu.index = pd.MultiIndex.from_arrays([dwlu.index, dwlu.actionId])\n",
    "\n",
    "dwlu = dwlu.drop(\"actionId\", axis =1)\n",
    "dwlu = dwlu.sort_values(\"startTime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "df_majority = dwlu[dwlu['isSTEM'] == 0]\n",
    "df_minority = dwlu[dwlu['isSTEM'] == 1]\n",
    "\n",
    "minority_len = len(df_minority.index.get_level_values(0).unique())\n",
    "\n",
    "majority_ids = df_majority.index.get_level_values(0).unique()\n",
    "sample_majority_ids = resample(majority_ids , n_samples=minority_len, replace=False).values\n",
    "\n",
    "sampled_df_majority = df_majority.sort_index(level=0).loc[sample_majority_ids, :]\n",
    "\n",
    "dwlu = pd.concat([df_minority, sampled_df_majority])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for excluding binary features from standardization process\n",
    "binary_cols = ['AveKnow', 'AveCarelessness', 'correct', 'original', 'hint', 'scaffold', 'bottomHint', 'frIsHelpRequest', 'stlHintUsed', 'frWorkingInSchool',\n",
    "               'responseIsFillIn', 'responseIsChosen', 'endsWithScaffolding', 'endsWithAutoScaffolding', 'frIsHelpRequestScaffolding', 'timeGreater5Secprev2wrong', 'helpAccessUnder2Sec', 'timeGreater10SecAndNextActionRight', 'timeOver80', 'manywrong']\n",
    "should_not_normalize_cols = ['isSTEM', 'RES_BORED', 'RES_CONCENTRATING', 'RES_CONFUSED', 'RES_FRUSTRATED', 'RES_OFFTASK', 'RES_GAMING']\n",
    "# also for 'AveCorrect', AveResBored, AveResEngcon, AveResConf, AveResFrust, AveResOfftask, AveResGaming, , Ln-1, Ln, \n",
    "should_not_normalized = dwlu[should_not_normalize_cols + binary_cols]\n",
    "should_normalized = dwlu.drop(should_not_normalize_cols + binary_cols, axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling necessary columns\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(should_normalized)\n",
    "scaled_data = DataFrame(scaled_data, index=should_normalized.index, columns=should_normalized.columns)\n",
    "scaled_dwlu = scaled_data.join(should_not_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_competition = scaled_dwlu.loc[unlabels.index.values, :].drop(\"isSTEM\", axis=1)\n",
    "x = scaled_dwlu.loc[labels.index.values, :].drop(\"isSTEM\", axis=1)\n",
    "y = scaled_dwlu.loc[labels.index.values, :][['isSTEM']].reset_index(level=1, drop=True)\n",
    "y = y[~y.index.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stud_id, stud_seq in dataset.groupby(\"ITEST_id\"):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepared dataset contains: per_stud_dataset + per_action_dataset_summ\n"
     ]
    }
   ],
   "source": [
    "pre = Preprocessing()\n",
    "_,_ = pre.load_data(time_gap=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "per_action_dataset = pre.per_action_dataset\n",
    "per_action_dataset.index = per_action_dataset.ITEST_id\n",
    "\n",
    "per_action_dataset = pre.per_action_dataset.drop(['ITEST_id', 'assistmentId', 'problemId', 'assignmentId'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "per_stud_dataset = pre.per_stud_dataset.drop(\"isSTEM\", axis=1)\n",
    "per_stud_dataset.index = per_stud_dataset.ITEST_id\n",
    "per_stud_dataset = per_stud_dataset.drop(\"ITEST_id\", axis=1)\n",
    "dataset = per_stud_dataset.join(per_action_dataset)\n",
    "dataset.index = pd.MultiIndex.from_arrays([dataset.index.values, dataset.actionId.values], names=[\"ITEST_id\", \"actionId\"])\n",
    "dataset = dataset.drop(\"actionId\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = pre.test_dataset.drop(\"ITEST_id\", axis=1)\n",
    "test_dataset.index = pre.test_dataset.ITEST_id\n",
    "\n",
    "shared_ids_with_train = test_dataset.index.intersection(dataset.index.get_level_values(0).unique().values)\n",
    "pure_testset = test_dataset.drop(shared_ids_with_train)\n",
    "raw_dataset = pre.raw_dataset.drop(Cols.excluded_cols + [\"ITEST_id\"], axis=1)\n",
    "raw_dataset.index = pre.raw_dataset.ITEST_id\n",
    "\n",
    "included_cols = list(set(Cols.per_action_cols + Cols.per_stud_cols).difference(set(['isSTEM', 'assistmentId', 'problemId', 'assignmentId'] + Cols.excluded_cols + Cols.per_stud_cols_cat + Cols.per_action_cols_cat)))\n",
    "pure_testset_perActionAndStud = pure_testset.join(raw_dataset)[included_cols]\n",
    "pure_testset_perActionAndStud.index = pd.MultiIndex.from_arrays([pure_testset_perActionAndStud.index.values,pure_testset_perActionAndStud.actionId.values ], names=[\"ITEST_id\", \"actionId\"])\n",
    "pure_testset_perActionAndStud = pure_testset_perActionAndStud.drop(\"actionId\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([dataset, pure_testset_perActionAndStud])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining sequence length (or number of time-steps) for each student and batch-size\n",
    "max_length_seq = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show the seq of a sample student (id =9)\n",
    "#per_action_dataset[per_action_dataset.index.get_level_values(0) ==9].head()\n",
    "# per_action_dataset.to_csv(\"Debug/1-per_action_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncating seq of each student and only considering his last 'max_length_seq' actions based on startTime feature\n",
    "truncated_input = dataset.sort_values(\"startTime\",ascending=True).groupby('ITEST_id').tail(max_length_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncated_input.to_csv(\"Debug/2-truncated_input.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for excluding binary features from standardization process\n",
    "binary_cols = ['AveKnow', 'AveCarelessness', 'correct', 'original', 'hint', 'scaffold', 'bottomHint', 'frIsHelpRequest', 'stlHintUsed', 'frWorkingInSchool',\n",
    "               'responseIsFillIn', 'responseIsChosen', 'endsWithScaffolding', 'endsWithAutoScaffolding', 'frIsHelpRequestScaffolding', 'timeGreater5Secprev2wrong', 'helpAccessUnder2Sec', 'timeGreater10SecAndNextActionRight', 'timeOver80', 'manywrong']\n",
    "should_not_normalize_cols = ['RES_BORED', 'RES_CONCENTRATING', 'RES_CONFUSED', 'RES_FRUSTRATED', 'RES_OFFTASK', 'RES_GAMING']\n",
    "# also for 'AveCorrect', AveResBored, AveResEngcon, AveResConf, AveResFrust, AveResOfftask, AveResGaming, , Ln-1, Ln, \n",
    "should_not_normalized = truncated_input[should_not_normalize_cols + binary_cols]\n",
    "should_normalized = truncated_input.drop(should_not_normalize_cols + binary_cols, axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# scaling dataset\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(should_normalized)\n",
    "scaled_data = DataFrame(scaled_data, index=should_normalized.index, columns=should_normalized.columns)\n",
    "X_scaled = scaled_data.join(should_not_normalized)\n",
    "\n",
    "# X_scaled[X_scaled.index.get_level_values(0) ==9].head()\n",
    "# X_scaled.to_csv(\"Debug/3-X_scaled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()  # create figure & 1 axis\n",
    "# X_scaled.boxplot(X_scaled.columns.difference([\"startTime\", 'endTime']).values.tolist(),ax=ax)\n",
    "# fig.set_size_inches(100,100)\n",
    "# fig.savefig('X_scaled.png')   # save the figure to file\n",
    "# plt.close(fig) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# padding sequences to have a same length\n",
    "\n",
    "X_padded = X_scaled\n",
    "\n",
    "# X_padded = []\n",
    "# for stud_id, stud_seq in X_scaled.groupby('ITEST_id'):\n",
    "#     X_padded.append(stud_seq)\n",
    "# # changing truncating from post to pre after inversing the seq. Also it makes more sense to change the padding from post to pre!    \n",
    "# X_padded = sequence.pad_sequences(X_padded, max_length_seq, dtype='float64', padding=\"pre\", truncating=\"pre\", value=999999.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# getting a sense about padded dataset\n",
    "# ITEST_ix = pd.Index(np.repeat(sorted(X_scaled.index.get_level_values(0).unique()), max_length_seq), name='ITEST_id')\n",
    "# new_seq_ix = pd.Index(list(range(0,max_length_seq))* X_padded.shape[0], name=\"seq_ix\")\n",
    "# paddedData = DataFrame(X_padded.reshape(X_padded.shape[0]*X_padded.shape[1], -1), index=[ITEST_ix, new_seq_ix], columns=X_scaled.columns)\n",
    "# paddedData.to_csv(\"Debug/4-paddedData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creating label dataset with index as ITEST_id values\n",
    "label_dataset = pre.label_dataset[['ITEST_id', 'isSTEM']]\n",
    "label_dataset.index = label_dataset.ITEST_id\n",
    "label_dataset = label_dataset.drop(\"ITEST_id\", axis = 1)\n",
    "#label_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'paddedData' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-c07fe324873c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# separating unlabeled data from labeled data and preparing competition data for prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_competition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaddedData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpure_testset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpaddedData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaddedData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpure_testset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_competition_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_competition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_competition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'paddedData' is not defined"
     ]
    }
   ],
   "source": [
    "# separating unlabeled data from labeled data and preparing competition data for prediction\n",
    "X_competition = paddedData.loc[pure_testset.index.unique().values, :]\n",
    "paddedData = paddedData.drop(pure_testset.index.unique().values)\n",
    "X_competition_arr = X_competition.values.reshape(-1, max_length_seq, X_competition.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# join label with dataset to make sure we have a consistent dataset\n",
    "# (of course before the training the label column will be droped from data)\n",
    "X_with_label = paddedData.join(label_dataset)\n",
    "#print(X_with_label.shape)\n",
    "# X_with_label.to_csv(\"Debug/5-X_with_label.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "df_majority = X_with_label[X_with_label['isSTEM'] == 0]\n",
    "df_minority = X_with_label[X_with_label['isSTEM'] == 1]\n",
    "\n",
    "minority_len = len(df_minority.index.get_level_values(0).unique())\n",
    "majority_len = len(df_majority.index.get_level_values(0).unique())\n",
    "\n",
    "sampled_df_majority = df_majority.loc[resample(df_majority.index.get_level_values(0).unique(), n_samples=minority_len, replace=False).values]\n",
    "\n",
    "balanced_X = pd.concat([df_minority, sampled_df_majority])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# y_prepared = X_with_label[~X_with_label.index.get_level_values(0).duplicated()]['isSTEM'].values\n",
    "# X_prepared = X_with_label.drop(\"isSTEM\", axis=1)\n",
    "# X_prepared_columns = X_prepared.columns\n",
    "# X_prepared = X_prepared.values.reshape((-1, max_length_seq, X_prepared.shape[1]))\n",
    "# DataFrame(X_prepared.reshape(X_prepared.shape[0] * X_prepared.shape[1], X_prepared.shape[2]), index=X_with_label.index, columns=X_prepared_columns).to_csv(\"Debug/6-X_prepared.csv\")\n",
    "\n",
    "#for balanced dataset only \n",
    "# y_prepared = balanced_X[~balanced_X.index.get_level_values(0).duplicated()]['isSTEM'].values\n",
    "# X_prepared = balanced_X.drop(\"isSTEM\", axis=1)\n",
    "# X_prepared_columns = X_prepared.columns\n",
    "# X_prepared = X_prepared.values.reshape((-1, max_length_seq, X_prepared.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prepared = balanced_X[~balanced_X.index.get_level_values(0).duplicated()]['isSTEM'].values\n",
    "X_prepared = balanced_X.drop(\"isSTEM\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_prepared.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[stud_seq.values for stud_id, stud_seq in X_prepared.groupby(\"ITEST_id\")][3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_prepared_df = DataFrame(X_prepared.reshape(X_prepared.shape[0] * X_prepared.shape[1], X_prepared.shape[2]), index=balanced_X.index, columns=X_prepared_columns)\n",
    "# X_prepared_df.to_csv(\"Debug/6-X_prepared.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Masking\n",
    "\n",
    "def create_LSTM_model(max_length_seq, feature_size):\n",
    "    model = Sequential()\n",
    "    model.add(Masking(mask_value=999999., input_shape=(max_length_seq, feature_size)))\n",
    "    model.add(LSTM(100, return_sequences=True))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "#   sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting train/test data \n",
    "X_train, X_test, y_train,y_test = train_test_split(X_prepared,y_prepared, stratify=y_prepared, train_size=.77)\n",
    "enc = OneHotEncoder()\n",
    "y_train = enc.fit_transform(y_train.reshape(-1,1)).toarray()\n",
    "y_test = enc.fit_transform(y_test.reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xx_train = DataFrame(X_train.reshape(X_train.shape[0] * X_train.shape[1], X_train.shape[2]))\n",
    "\n",
    "# xx_train['label'] = np.repeat(np.argmax(y_train, axis=1), 500)\n",
    "\n",
    "# xx_train.to_csv(\"xx_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard, EarlyStopping\n",
    "tensorboard_callback = TensorBoard(log_dir='./logs', histogram_freq=1, batch_size=batch_size, write_graph=True, write_grads=True, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', min_delta=0.005, patience=5, verbose=10, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = create_LSTM_model(max_length_seq, X_train.shape[2])\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=batch_size, validation_data= (X_test,y_test), callbacks=[tensorboard_callback, early_stopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from Vis import plot_accuracy\n",
    "# do prediction\n",
    "y_pred = model.predict(X_test, batch_size=batch_size)\n",
    "\n",
    "# calculating scores\n",
    "roc_score = roc_auc_score(y_test, y_pred)\n",
    "# roc_score = roc_auc_score(np.argmax(y_test,axis=1), np.argmax(y_pred,axis=1))\n",
    "\n",
    "mse_score = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "#printing and plotting model and score information\n",
    "plot_loss(history)\n",
    "# plot_roc(y_test, y_pred)\n",
    "plot_roc(np.argmax(y_test,axis=1), np.argmax(y_pred,axis=1))\n",
    "plot_accuracy(history)\n",
    "\n",
    "print(model.summary())\n",
    "print(\"Test ROC Score: %f\" % roc_score)\n",
    "print(\"Test RMSE Score: %f\" % sqrt(mse_score))\n",
    "print(\"Final Competition Score: %f\" % (1 - sqrt(mse_score) + roc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_competition = model.predict(X_competition_arr, batch_size=batch_size)\n",
    "\n",
    "# result_index = X_competition.reset_index(level=1, drop=True).index.unique()\n",
    "\n",
    "# argmax_preds = [np.argmax(predicted_label) for predicted_label in y_pred_competition]\n",
    "\n",
    "# result_df = DataFrame(argmax_preds, index=pd.Index(result_index, name='ITEST_id'), columns=['isSTEM'])\n",
    "\n",
    "# final_output = pd.concat([result_df, label_dataset.loc[shared_ids_with_train.values]]).sort_index()\n",
    "# final_output.to_csv(\"submition_1_{}.csv\".format(theNotebook))\n",
    "# final_output.isSTEM.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
