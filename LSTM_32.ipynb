{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changelog:\n",
    "\n",
    "* v8 Checking code and revising the debug files. Ready for debugging \n",
    "* V9 Adding resampling\n",
    "* V10 Adding tensorboard\n",
    "* V11 Reducing LSTM layer to 1 in order to have better interpretation in tensorboard\n",
    "* V12 adding actionId to data in order to debug datasets and step better + fixing the wrong column name for x_prepared + **fixing LABEL WRONG!!!!! ASSIENMENT **\n",
    "* V13 removing masking layer and padding with 0 or 9999\n",
    "* V14 removing two outputs\n",
    "* V15 returning the masking layer with mask value 99.\n",
    "    - The result is that without adding masking layer the training accuracy does not increase at all and it remains the same on nearly 50\n",
    "    - however the validation accuracy remains the same around 50 and does not increase while the validation loss is being increased as allways\n",
    "* V16 Changing the optimizer to SGD\n",
    "* V17 Changing the optimizer to rmsprop\n",
    "* V18 Increasing batch size from 1 to higher and coming back the optimizer to Adam (both roc and accuracy on train was about .9 while the accuracy of validation was 0.5\n",
    "* V19 Separating validation set from data and feed to the fit function using validation_data param\n",
    "* V20 Adding another layer of 100 unit\n",
    "* V21 Joining per stud features\n",
    "* V22 removing the sampling\n",
    "* V23 Add two output\n",
    "* V24 Assumes that we are overfitting so we are going to:\n",
    "    - Remove additional LSTM layer to simplify the model $\\checkmark$\n",
    "    - Reduce the number of units for LSTM layer $\\checkmark$ (saw that finally validation loss starts to deacrease with 20 unit and 200 seq lenght)\n",
    "    - Reduce the number of features as much as possible (should be done after dinormalizing the binary variables)\n",
    "    - Reduce the number of seq length $\\checkmark$\n",
    "* V25 Excluding binary variables and some other proportional variables from the standardization step (it seems we are improving!)\n",
    "* V26 Adding competition validation set for making prediction on un-labeled data\n",
    "* Bringing resampling back to the game!\n",
    "* V28 Adding Startify spliting\n",
    "* V29 Inversing the sequence! (it make more sence to feed the old frames first and then go forward for newer frames)\n",
    "* V30 A revolutionary update: removing the padding and updating the weights with variable lenght sequences!\n",
    "* V31 Testing various model architecture and layers with different number of units\n",
    "    - V31.1 Disabling resampling\n",
    "* V32 Exclude students with large number of actions e.g. greater than 2000\n",
    "* V33 Encode categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "var kernel = IPython.notebook.kernel;\n",
       "var thename = window.document.getElementById(\"notebook_name\").innerHTML;\n",
       "var command = \"theNotebook = \" + \"'\"+thename+\"'\";\n",
       "kernel.execute(command);"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "var kernel = IPython.notebook.kernel;\n",
    "var thename = window.document.getElementById(\"notebook_name\").innerHTML;\n",
    "var command = \"theNotebook = \" + \"'\"+thename+\"'\";\n",
    "kernel.execute(command);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import TensorBoard, EarlyStopping\n",
    "from keras.preprocessing import sequence\n",
    "from keras import optimizers\n",
    "\n",
    "from Vis import plot_loss, plot_roc, plot_accuracy\n",
    "from Preprocessing import Preprocessing, Cols\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler, StandardScaler\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from pandas import DataFrame, Series\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "# pandas.set_option('max_columns',10)\n",
    "\n",
    "import os\n",
    "import glob\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loading datasets and labels\n",
    "data_files = glob.glob(os.path.join(\"Dataset\", \"student_log_*.csv\"))\n",
    "raw_dataset = pd.concat((pd.read_csv(f, index_col=[\"ITEST_id\"]) for f in data_files))\n",
    "\n",
    "dataset = raw_dataset.drop(Cols.excluded_cols + Cols.cat_cols, axis=1)\n",
    "\n",
    "labels = DataFrame.from_csv(\"Dataset/training_label.csv\")\n",
    "valid_test_label_dataset = DataFrame.from_csv(\"Dataset/validation_test_label.csv\")\n",
    "\n",
    "unlabels = valid_test_label_dataset.drop(list(labels.index.intersection(valid_test_label_dataset.index)))\n",
    "labels_unlabels = pd.concat([labels,unlabels])\n",
    "\n",
    "dwlu = dataset.join(labels_unlabels, how=\"inner\") # dwlu = dataset_with_labels_unlabels\n",
    "\n",
    "dwlu.index = pd.MultiIndex.from_arrays([dwlu.index, dwlu.actionId])\n",
    "\n",
    "dwlu = dwlu.drop(\"actionId\", axis =1)\n",
    "dwlu = dwlu.sort_values(\"startTime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219 students are removed!\n"
     ]
    }
   ],
   "source": [
    "# Excluding students with large number of actions (does not matter whether they are isSTEM=1 or not but does matter if they are isSTEM=NAN)\n",
    "isLarge = (dwlu.groupby(\"ITEST_id\").size() > 500)\n",
    "largeStuds_ids = isLarge[isLarge == True].index.values\n",
    "largeStuds_ids_with_label = [l for l in largeStuds_ids if l not in unlabels.index.values]\n",
    "\n",
    "\n",
    "print(\"%d students are removed!\" % len(largeStuds_ids_with_label))\n",
    "dwlu = dwlu.drop(largeStuds_ids_with_label, level=0)\n",
    "\n",
    "# no unlabeled data should be removed\n",
    "assert(len(dwlu[dwlu.isSTEM.isnull()].index.get_level_values(0).unique()) == len(unlabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling data in order to balance class labels\n",
    "from sklearn.utils import resample\n",
    "\n",
    "df_majority = dwlu[dwlu['isSTEM'] == 0]\n",
    "df_minority = dwlu[dwlu['isSTEM'] == 1]\n",
    "\n",
    "minority_len = len(df_minority.index.get_level_values(0).unique())\n",
    "\n",
    "majority_ids = df_majority.index.get_level_values(0).unique()\n",
    "sample_majority_ids = resample(majority_ids , n_samples=minority_len, replace=False).values\n",
    "\n",
    "sampled_df_majority = df_majority.sort_index(level=0).loc[sample_majority_ids, :]\n",
    "\n",
    "dwlu = pd.concat([df_minority, sampled_df_majority])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying columns that should be normalized\n",
    "binary_cols = ['AveKnow', 'AveCarelessness', 'correct', 'original', 'hint', 'scaffold', 'bottomHint', 'frIsHelpRequest', 'stlHintUsed', 'frWorkingInSchool',\n",
    "               'responseIsFillIn', 'responseIsChosen', 'endsWithScaffolding', 'endsWithAutoScaffolding', 'frIsHelpRequestScaffolding', 'timeGreater5Secprev2wrong', 'helpAccessUnder2Sec', 'timeGreater10SecAndNextActionRight', 'timeOver80', 'manywrong']\n",
    "should_not_normalize_cols = ['isSTEM', 'RES_BORED', 'RES_CONCENTRATING', 'RES_CONFUSED', 'RES_FRUSTRATED', 'RES_OFFTASK', 'RES_GAMING']\n",
    "# also for 'AveCorrect', 'AveResBored', 'AveResEngcon', 'AveResConf', 'AveResFrust', 'AveResOfftask', 'AveResGaming', 'Ln-1', 'Ln', \n",
    "should_not_normalized = dwlu[should_not_normalize_cols + binary_cols]\n",
    "should_normalized = dwlu.drop(should_not_normalize_cols + binary_cols, axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling necessary columns\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(should_normalized)\n",
    "scaled_data = DataFrame(scaled_data, index=should_normalized.index, columns=should_normalized.columns)\n",
    "scaled_dwlu = scaled_data.join(should_not_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building learning datasets/labels and competition preditcion dataset\n",
    "x_competition = scaled_dwlu.loc[unlabels.index.values, :].drop(\"isSTEM\", axis=1)\n",
    "x = scaled_dwlu.loc[labels.index.values, :].drop(\"isSTEM\", axis=1)\n",
    "y = scaled_dwlu.loc[labels.index.values, :][['isSTEM']].reset_index(level=1, drop=True)\n",
    "y = y[~y.index.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting X from a 2D dataframe to a 3D list of varibale-lenght sequecens\n",
    "x = np.array([stud_seq.values for _, stud_seq in x.groupby(\"ITEST_id\")])\n",
    "y = y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting train/test data \n",
    "# TODO test Startify\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding train/test labels\n",
    "enc = OneHotEncoder()\n",
    "y_train = enc.fit_transform(y_train.reshape(-1,1)).toarray()\n",
    "y_test = enc.fit_transform(y_test.reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining model configs.\n",
    "batch_size = 1 # sequences are independent and the network weights should be updated after each sequence\n",
    "feature_size = x_train[0].shape[1] # num. of features extracted from first training sample\n",
    "timestep_size = None # here we are using variable-length sequences so there is no fixed timestep size\n",
    "nb_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating necessary callbacks for early stopping and Tensorboard visualization\n",
    "tensorboard_callback = TensorBoard(log_dir='./logs', histogram_freq=1, batch_size=batch_size, write_graph=True, write_grads=True, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', min_delta=0.005, patience=5, verbose=10, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a variable length model!\n",
    "def create_LSTM_model(feature_size):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(200, input_shape=(None, feature_size)))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5204 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 0.7411 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.6488 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 533ms/step - loss: 0.6717 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 226ms/step - loss: 0.8906 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.9479 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 649ms/step - loss: 1.1850 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 0.7745 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 0.7035 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 0.6545 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 0.4137 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 324ms/step - loss: 0.9380 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 1.0426 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.8030 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 1.9254 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.4464 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 1.3054 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.3599 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 0.2691 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 1.0524 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 639ms/step - loss: 0.5319 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 0.7176 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.4307 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.8138 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.5298 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 0.7444 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.7204 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 0.7104 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.7989 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.6935 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 647ms/step - loss: 0.8467 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 0.7044 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.1707 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 0.3719 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.5308 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 0.5375 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 1.0263 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 1.5304 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 0.9558 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 393ms/step - loss: 0.4958 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 0.4369 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 245ms/step - loss: 0.8903 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.4391 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.2398 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 158ms/step - loss: 1.2302 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 1.1009 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 1.6205 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 4.1078 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.4135 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 254ms/step - loss: 0.4351 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 1.1955 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 0.5551 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.2208 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.6144 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 0.5621 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 1.0031 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.8502 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 0.7189 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.5530 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 0.9722 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 1.2979 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.5070 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 0.5579 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 0.5473 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 0.8497 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 0.6205 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 0.4918 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 1.5556 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.5197 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 0.3333 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 233ms/step - loss: 0.6840 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 345ms/step - loss: 0.4747 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.5380 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.2991 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.1476 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 0.5446 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.1074 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 519ms/step - loss: 0.5880 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.6362 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.6959 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.2186 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 416ms/step - loss: 1.4242 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 254ms/step - loss: 0.2269 - acc: 1.0000\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "val_loss for each sample at the end of epoch: [1.6371263265609741, 0.0]\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "val_loss for each sample at the end of epoch: [1.0206420421600342, 0.0]\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "val_loss for each sample at the end of epoch: [1.9306769371032715, 0.0]\n",
      "1/1 [==============================] - 0s 9ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss for each sample at the end of epoch: [0.54773473739624023, 1.0]\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "val_loss for each sample at the end of epoch: [0.49151545763015747, 1.0]\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "val_loss for each sample at the end of epoch: [0.15333814918994904, 1.0]\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "val_loss for each sample at the end of epoch: [1.2852237224578857, 0.0]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "val_loss for each sample at the end of epoch: [0.74909579753875732, 0.0]\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "val_loss for each sample at the end of epoch: [0.48977500200271606, 1.0]\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "val_loss for each sample at the end of epoch: [0.23312652111053467, 1.0]\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "val_loss for each sample at the end of epoch: [0.46003299951553345, 1.0]\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "val_loss for each sample at the end of epoch: [1.0953466892242432, 0.0]\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "val_loss for each sample at the end of epoch: [1.1534152030944824, 0.0]\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "val_loss for each sample at the end of epoch: [0.43726184964179993, 1.0]\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "val_loss for each sample at the end of epoch: [0.65308177471160889, 1.0]\n",
      "epoch: 1\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 0.0216 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 229ms/step - loss: 1.8053 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0462 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 539ms/step - loss: 1.1670 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 0.4354 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 1.9450 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 657ms/step - loss: 0.4200 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 0.5363 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 0.4567 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 0.3636 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 1.0102 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 1.5290 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 0.8035 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.5951 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 1.6564 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.5627 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 0.5762 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.3051 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 0.5071 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 1.1856 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 641ms/step - loss: 0.4275 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 0.3585 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.2065 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.7964 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 0.6088 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 0.4563 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.6265 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 0.4216 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.8061 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.7036 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 654ms/step - loss: 0.4531 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 0.8115 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.4528 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 0.3390 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.4906 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 0.4555 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 0.4404 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 0.7657 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 1.0886 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 394ms/step - loss: 0.3348 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.2539 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 0.3828 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.4190 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.1648 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 1.0260 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.9078 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 0.8521 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 0.9938 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.6037 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 254ms/step - loss: 0.2176 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.9197 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 0.3618 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.1374 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.7589 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 0.5871 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.8412 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6550 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 0.7515 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.5300 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 0.5239 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 0.9126 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.6355 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 0.3864 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 0.4345 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 1.2269 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 0.5127 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 0.4980 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 1.1299 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.6169 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 0.3378 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 235ms/step - loss: 0.8356 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 344ms/step - loss: 0.2859 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.4243 - acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.3649 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.6357 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 0.5002 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.1060 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 512ms/step - loss: 0.3384 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.6518 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.4475 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 0.3207 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 416ms/step - loss: 1.0193 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 250ms/step - loss: 0.5123 - acc: 1.0000\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "val_loss for each sample at the end of epoch: [1.4594504833221436, 0.0]\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "val_loss for each sample at the end of epoch: [1.0169357061386108, 0.0]\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "val_loss for each sample at the end of epoch: [1.162214994430542, 0.0]\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "val_loss for each sample at the end of epoch: [0.74924886226654053, 0.0]\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "val_loss for each sample at the end of epoch: [0.40302440524101257, 1.0]\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "val_loss for each sample at the end of epoch: [0.17381370067596436, 1.0]\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "val_loss for each sample at the end of epoch: [1.2156298160552979, 0.0]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "val_loss for each sample at the end of epoch: [0.87632095813751221, 0.0]\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "val_loss for each sample at the end of epoch: [0.32210361957550049, 1.0]\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "val_loss for each sample at the end of epoch: [0.38023373484611511, 1.0]\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "val_loss for each sample at the end of epoch: [0.36041933298110962, 1.0]\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "val_loss for each sample at the end of epoch: [1.1961052417755127, 0.0]\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "val_loss for each sample at the end of epoch: [0.68451672792434692, 1.0]\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "val_loss for each sample at the end of epoch: [0.65883368253707886, 1.0]\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "val_loss for each sample at the end of epoch: [0.4301077127456665, 1.0]\n",
      "epoch: 2\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 0.0168 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 0.5838 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0464 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 539ms/step - loss: 1.0086 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 0.4892 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 1.5335 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 645ms/step - loss: 0.4504 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 0.5105 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 0.4611 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 0.2480 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 0.6769 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 324ms/step - loss: 1.6157 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 0.6655 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.6221 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 1.6459 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.6135 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 0.3447 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.3311 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 0.5455 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 1.0776 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 636ms/step - loss: 0.4465 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 0.1999 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.2029 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4929 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 0.6171 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 0.4437 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 0.8324 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 0.4019 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.6544 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3582 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 655ms/step - loss: 0.3583 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.7811 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.4513 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 0.3446 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.1903 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 0.3714 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 0.2247 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 0.5025 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 1.1907 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 391ms/step - loss: 0.1935 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 0.1570 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 0.1350 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.3645 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0836 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.8430 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.8963 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 0.6493 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 324ms/step - loss: 0.3148 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.7417 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 254ms/step - loss: 0.0796 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.5326 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 0.2300 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0748 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.6640 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.6604 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.7671 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.4884 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 0.8384 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.4656 - acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 236ms/step - loss: 0.2715 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 0.6582 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.6044 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 0.2877 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 0.2766 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 1.5213 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 0.4136 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 0.4993 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 1.1361 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.1699 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 0.2220 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 233ms/step - loss: 0.7850 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 347ms/step - loss: 0.1889 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.2181 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.2099 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.5297 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 0.3006 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0590 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 514ms/step - loss: 0.2402 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 0.7701 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.2991 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 0.2607 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 418ms/step - loss: 0.8028 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 0.9819 - acc: 0.0000e+00\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "val_loss for each sample at the end of epoch: [1.4106810092926025, 0.0]\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "val_loss for each sample at the end of epoch: [0.95192956924438477, 0.0]\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "val_loss for each sample at the end of epoch: [1.0180306434631348, 0.0]\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "val_loss for each sample at the end of epoch: [1.4109375476837158, 0.0]\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "val_loss for each sample at the end of epoch: [0.27340590953826904, 1.0]\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "val_loss for each sample at the end of epoch: [0.073303192853927612, 1.0]\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "val_loss for each sample at the end of epoch: [1.2667880058288574, 0.0]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "val_loss for each sample at the end of epoch: [1.0825748443603516, 0.0]\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "val_loss for each sample at the end of epoch: [0.20777571201324463, 1.0]\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "val_loss for each sample at the end of epoch: [0.36822593212127686, 1.0]\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "val_loss for each sample at the end of epoch: [0.3628307580947876, 1.0]\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "val_loss for each sample at the end of epoch: [1.2905669212341309, 0.0]\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "val_loss for each sample at the end of epoch: [0.55612993240356445, 1.0]\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "val_loss for each sample at the end of epoch: [0.79716378450393677, 0.0]\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "val_loss for each sample at the end of epoch: [0.45117434859275818, 1.0]\n",
      "epoch: 3\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 0.0085 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 0.1187 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0292 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 535ms/step - loss: 0.8486 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 226ms/step - loss: 0.5286 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 1.1015 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 647ms/step - loss: 0.5013 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 0.6090 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 0.3869 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 0.1834 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 0.3296 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 1.4978 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.4028 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.4104 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 1.9046 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.3854 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 0.1069 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.3353 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 0.3113 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 1.2124 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 626ms/step - loss: 0.3476 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 0.0449 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.1393 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3973 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 0.8020 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 0.3245 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.7296 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 0.3017 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.2155 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.1642 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 651ms/step - loss: 0.2182 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 0.9151 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.3378 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 0.3438 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.1206 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 0.2611 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 0.1482 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 0.4024 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 0.8758 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 393ms/step - loss: 0.0834 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 0.1194 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 250ms/step - loss: 0.0477 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.2642 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0541 - acc: 1.0000\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 161ms/step - loss: 0.5195 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.4761 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.5394 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 0.0781 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.8173 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 0.0191 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.1164 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 0.0779 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0482 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3426 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 0.7284 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.8075 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.3290 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 0.8343 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.4175 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 0.0751 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 0.4276 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.1118 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 0.2076 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 0.1217 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 1.7214 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 0.1903 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.6044 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 1.0678 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0192 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 0.2475 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 235ms/step - loss: 0.5757 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 345ms/step - loss: 0.1049 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.1194 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.0377 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.1229 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 0.1160 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.0151 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 516ms/step - loss: 0.3127 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.3940 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.1375 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.2201 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 419ms/step - loss: 0.4659 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 250ms/step - loss: 0.8855 - acc: 0.0000e+00\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "val_loss for each sample at the end of epoch: [0.74465978145599365, 0.0]\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "val_loss for each sample at the end of epoch: [1.2123650312423706, 0.0]\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "val_loss for each sample at the end of epoch: [1.5492910146713257, 0.0]\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "val_loss for each sample at the end of epoch: [1.4554648399353027, 0.0]\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "val_loss for each sample at the end of epoch: [0.19906574487686157, 1.0]\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "val_loss for each sample at the end of epoch: [0.029263241216540337, 1.0]\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "val_loss for each sample at the end of epoch: [1.0887759923934937, 0.0]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "val_loss for each sample at the end of epoch: [1.1223819255828857, 0.0]\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "val_loss for each sample at the end of epoch: [0.067668862640857697, 1.0]\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "val_loss for each sample at the end of epoch: [0.53875935077667236, 1.0]\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "val_loss for each sample at the end of epoch: [0.25276809930801392, 1.0]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "val_loss for each sample at the end of epoch: [1.333005428314209, 0.0]\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "val_loss for each sample at the end of epoch: [0.46746182441711426, 1.0]\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "val_loss for each sample at the end of epoch: [1.2945663928985596, 0.0]\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "val_loss for each sample at the end of epoch: [0.3665693998336792, 1.0]\n",
      "epoch: 4\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 0.0207 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0039 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 533ms/step - loss: 0.1808 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 226ms/step - loss: 0.8095 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.5388 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 645ms/step - loss: 0.4473 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 1.2487 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 0.2014 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 0.1766 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 0.1159 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 1.5490 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 0.1168 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.4569 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 1.6102 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.2600 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 0.0924 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0640 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 340ms/step - loss: 0.3994 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.7839 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 642ms/step - loss: 0.0697 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 0.1057 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0073 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.7107 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 0.4932 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 0.7742 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 1.1497 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 0.6759 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.1097 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0822 - acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 651ms/step - loss: 0.3082 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.6906 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.5173 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 0.3905 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0944 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 0.2963 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.6670 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 0.1543 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 0.3838 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 394ms/step - loss: 0.0664 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 0.0948 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 0.1956 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.1795 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.1251 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.5804 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.3224 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 0.4972 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 0.1165 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.6314 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 254ms/step - loss: 0.0370 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.3185 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 0.1989 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0364 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.3893 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.5051 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.3610 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.2180 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 0.8204 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.4558 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 0.0399 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 0.0520 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0342 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 0.2615 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 0.1243 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 1.9091 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 0.0376 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 0.3178 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 0.8534 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0873 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 0.0751 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 237ms/step - loss: 0.8345 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 343ms/step - loss: 0.0507 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.4614 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1090 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.1004 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 0.1226 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.4422 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 516ms/step - loss: 0.1272 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.3858 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.2912 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 0.1766 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 417ms/step - loss: 0.2233 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 0.2215 - acc: 1.0000\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "val_loss for each sample at the end of epoch: [1.1442546844482422, 0.0]\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "val_loss for each sample at the end of epoch: [0.69995963573455811, 0.0]\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "val_loss for each sample at the end of epoch: [2.4547991752624512, 0.0]\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "val_loss for each sample at the end of epoch: [0.81345432996749878, 0.0]\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "val_loss for each sample at the end of epoch: [0.26391670107841492, 1.0]\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "val_loss for each sample at the end of epoch: [0.013498226180672646, 1.0]\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "val_loss for each sample at the end of epoch: [1.6848526000976562, 0.0]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "val_loss for each sample at the end of epoch: [1.5299069881439209, 0.0]\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "val_loss for each sample at the end of epoch: [0.27849125862121582, 1.0]\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "val_loss for each sample at the end of epoch: [0.6847795844078064, 1.0]\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "val_loss for each sample at the end of epoch: [0.30236843228340149, 1.0]\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "val_loss for each sample at the end of epoch: [0.86720764636993408, 0.0]\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "val_loss for each sample at the end of epoch: [0.85159480571746826, 0.0]\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "val_loss for each sample at the end of epoch: [0.51981443166732788, 1.0]\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "val_loss for each sample at the end of epoch: [0.32459980249404907, 1.0]\n",
      "epoch: 5\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 0.0061 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 226ms/step - loss: 0.0211 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0039 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 540ms/step - loss: 0.1369 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 0.1541 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 1.1041 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 651ms/step - loss: 0.1538 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 0.7484 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 0.6140 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 0.1371 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 0.0261 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 0.8624 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 1.5067 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.6285 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 1.9110 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.1184 - acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 0.1886 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.1499 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 0.2666 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.8710 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 634ms/step - loss: 0.0481 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 0.0502 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0104 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.1554 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 254ms/step - loss: 0.1012 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 0.2201 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.9879 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 0.2913 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.1928 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0276 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 648ms/step - loss: 0.2370 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.5620 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.0258 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 0.5509 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.0187 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 0.7797 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 0.0315 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 0.1343 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 0.2670 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 396ms/step - loss: 0.0662 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 0.0397 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 0.0264 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.1834 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.0246 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.1704 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.1764 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 0.1790 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 0.0291 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.4573 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 0.0737 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0712 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 0.0914 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0282 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.1535 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 0.4032 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 1.1872 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0160 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 0.7330 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.2438 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 0.0850 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 325ms/step - loss: 0.2234 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 0.0102 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 0.2144 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 0.0486 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 1.1390 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 0.0158 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 0.1116 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 0.9804 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0187 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 0.0580 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 234ms/step - loss: 0.3504 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 350ms/step - loss: 0.0617 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0556 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.0406 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0142 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 0.1114 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.1383 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 512ms/step - loss: 0.0863 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.8985 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.0352 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 0.2365 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 417ms/step - loss: 0.0350 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 0.0498 - acc: 1.0000\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "val_loss for each sample at the end of epoch: [0.95234775543212891, 0.0]\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "val_loss for each sample at the end of epoch: [0.55402290821075439, 1.0]\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "val_loss for each sample at the end of epoch: [1.0470646619796753, 0.0]\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "val_loss for each sample at the end of epoch: [0.058452792465686798, 1.0]\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "val_loss for each sample at the end of epoch: [0.020091913640499115, 1.0]\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "val_loss for each sample at the end of epoch: [0.010230941697955132, 1.0]\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "val_loss for each sample at the end of epoch: [0.61859899759292603, 1.0]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "val_loss for each sample at the end of epoch: [3.3001768589019775, 0.0]\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "val_loss for each sample at the end of epoch: [0.019752718508243561, 1.0]\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "val_loss for each sample at the end of epoch: [1.063753604888916, 0.0]\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "val_loss for each sample at the end of epoch: [0.15101538598537445, 1.0]\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "val_loss for each sample at the end of epoch: [1.5402148962020874, 0.0]\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "val_loss for each sample at the end of epoch: [0.50548219680786133, 1.0]\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "val_loss for each sample at the end of epoch: [1.7678627967834473, 0.0]\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "val_loss for each sample at the end of epoch: [0.2521207332611084, 1.0]\n",
      "epoch: 6\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 0.0102 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 0.0426 - acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0059 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 537ms/step - loss: 0.0213 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 0.9882 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.2367 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 646ms/step - loss: 0.0711 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 0.2996 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 0.2552 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 0.0146 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.0371 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 1.7883 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 0.0412 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0116 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 1.8126 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0229 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 0.0260 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.3631 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 0.0261 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.2263 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 646ms/step - loss: 0.0222 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 0.0441 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0122 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.0760 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.1021 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 0.1801 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.5728 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 0.2081 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0230 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.7900 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 648ms/step - loss: 0.0687 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.5222 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0443 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 0.3507 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.2153 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 0.1242 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 0.5141 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 0.0242 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 0.1186 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 403ms/step - loss: 0.0723 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 0.0261 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 245ms/step - loss: 0.0480 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.3969 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0525 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.0244 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 1.2204 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 0.0834 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 0.0099 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 0.8380 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 254ms/step - loss: 0.0091 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0169 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 230ms/step - loss: 0.0046 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0085 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.2104 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 0.6969 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.6613 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.5700 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 0.3820 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.1865 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 0.1704 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 0.0790 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0917 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 0.2108 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 0.0859 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 1.4291 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 0.0556 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 0.0586 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 0.7098 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0235 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 0.1767 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 232ms/step - loss: 0.5590 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 342ms/step - loss: 0.0548 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0228 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.0277 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0611 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 0.3283 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.0186 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 514ms/step - loss: 0.0598 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.8390 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0462 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 0.4176 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 416ms/step - loss: 0.0765 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 252ms/step - loss: 0.0335 - acc: 1.0000\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "val_loss for each sample at the end of epoch: [1.8953394889831543, 0.0]\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "val_loss for each sample at the end of epoch: [1.1069386005401611, 0.0]\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "val_loss for each sample at the end of epoch: [1.0631527900695801, 0.0]\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "val_loss for each sample at the end of epoch: [0.10195564478635788, 1.0]\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "val_loss for each sample at the end of epoch: [0.056232579052448273, 1.0]\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "val_loss for each sample at the end of epoch: [0.10240930318832397, 1.0]\n",
      "1/1 [==============================] - 0s 10ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss for each sample at the end of epoch: [1.4261586666107178, 0.0]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "val_loss for each sample at the end of epoch: [2.5056290626525879, 0.0]\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "val_loss for each sample at the end of epoch: [0.018792303279042244, 1.0]\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "val_loss for each sample at the end of epoch: [1.0968778133392334, 0.0]\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "val_loss for each sample at the end of epoch: [0.19556331634521484, 1.0]\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "val_loss for each sample at the end of epoch: [1.5443477630615234, 0.0]\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "val_loss for each sample at the end of epoch: [0.074646323919296265, 1.0]\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "val_loss for each sample at the end of epoch: [2.6211574077606201, 0.0]\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "val_loss for each sample at the end of epoch: [0.34178224205970764, 1.0]\n",
      "epoch: 7\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 0.0085 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 0.1479 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.0040 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 532ms/step - loss: 0.1946 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 0.3659 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.1689 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 655ms/step - loss: 0.0285 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.3183 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 0.0718 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 0.0778 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 0.0119 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 0.4841 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 0.3244 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0532 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 1.2189 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.0212 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 0.0067 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0424 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 0.0175 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.3218 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 638ms/step - loss: 0.0173 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 0.0070 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0135 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.0153 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1145 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 0.0822 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.4208 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 0.0762 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0046 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 648ms/step - loss: 0.0460 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.4246 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.2170 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 0.3621 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 0.2291 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 0.1083 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 0.0317 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 0.8405 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 396ms/step - loss: 0.0146 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 0.0042 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 246ms/step - loss: 0.0099 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.0135 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0114 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 0.0415 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 0.4438 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 0.0198 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 0.0607 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 1.5094 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 0.0054 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0208 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 0.0123 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0195 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0152 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 0.0528 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 0.7865 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.0090 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 0.3855 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1275 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 0.0514 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 0.1468 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0075 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 0.2211 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 0.0391 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 0.2600 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 0.1163 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.0422 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 0.7471 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0070 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 0.0634 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 234ms/step - loss: 0.0799 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 345ms/step - loss: 0.0347 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0045 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.0277 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0093 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 0.0651 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.3066 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 502ms/step - loss: 0.0488 - acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 0.2282 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0116 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 0.3831 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 421ms/step - loss: 0.0259 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 0.0370 - acc: 1.0000\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "val_loss for each sample at the end of epoch: [0.32573145627975464, 1.0]\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "val_loss for each sample at the end of epoch: [0.092423021793365479, 1.0]\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "val_loss for each sample at the end of epoch: [0.23918600380420685, 1.0]\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "val_loss for each sample at the end of epoch: [1.4958043098449707, 0.0]\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "val_loss for each sample at the end of epoch: [0.18657582998275757, 1.0]\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "val_loss for each sample at the end of epoch: [0.25074398517608643, 1.0]\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "val_loss for each sample at the end of epoch: [2.1698791980743408, 0.0]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "val_loss for each sample at the end of epoch: [0.55610561370849609, 1.0]\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "val_loss for each sample at the end of epoch: [0.0067409230396151543, 1.0]\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "val_loss for each sample at the end of epoch: [0.58875691890716553, 1.0]\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "val_loss for each sample at the end of epoch: [0.10518050938844681, 1.0]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "val_loss for each sample at the end of epoch: [1.6265864372253418, 0.0]\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "val_loss for each sample at the end of epoch: [0.035544335842132568, 1.0]\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "val_loss for each sample at the end of epoch: [4.6058177947998047, 0.0]\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "val_loss for each sample at the end of epoch: [0.1497548520565033, 1.0]\n",
      "epoch: 8\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 0.0060 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.0062 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 533ms/step - loss: 0.0333 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 0.0627 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.0198 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 654ms/step - loss: 0.0100 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 0.1265 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 0.0195 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 1.4130 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 0.0062 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 0.1485 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 1.9946 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0178 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 1.1920 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.4303 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 0.3468 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.0188 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 0.0299 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 114ms/step - loss: 0.3448 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 632ms/step - loss: 0.0047 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 0.0707 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.0289 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 0.0949 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 0.7813 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 2.3857 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 1.1537 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.1759 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0060 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 655ms/step - loss: 0.2152 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 0.1235 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.0050 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 1.0914 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.0174 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 1.3229 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.1011 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 0.1487 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 1.0570 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 394ms/step - loss: 0.0529 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 0.0205 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 0.0248 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0610 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0248 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.0717 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.0975 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 0.3019 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 0.0329 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.2893 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 254ms/step - loss: 0.0358 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0148 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 0.0219 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 0.0853 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0273 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 0.1479 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.1596 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0055 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 0.4820 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.1909 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 0.0570 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 0.3866 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 1.5977 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 0.0655 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 0.0359 - acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 0.9142 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 0.9143 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.0954 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.6131 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0289 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 0.1756 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 237ms/step - loss: 0.5356 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 341ms/step - loss: 0.0541 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0078 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.0525 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0111 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 337ms/step - loss: 0.0406 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0333 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 513ms/step - loss: 0.0704 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.6540 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0712 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 0.0801 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 414ms/step - loss: 0.1845 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 0.0218 - acc: 1.0000\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "val_loss for each sample at the end of epoch: [1.3369326591491699, 0.0]\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "val_loss for each sample at the end of epoch: [1.4762177467346191, 0.0]\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "val_loss for each sample at the end of epoch: [0.44405794143676758, 1.0]\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "val_loss for each sample at the end of epoch: [0.10019631683826447, 1.0]\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "val_loss for each sample at the end of epoch: [0.10517227649688721, 1.0]\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "val_loss for each sample at the end of epoch: [0.79733359813690186, 0.0]\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "val_loss for each sample at the end of epoch: [1.5170578956604004, 0.0]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "val_loss for each sample at the end of epoch: [1.6353998184204102, 0.0]\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "val_loss for each sample at the end of epoch: [0.070300959050655365, 1.0]\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "val_loss for each sample at the end of epoch: [1.1843874454498291, 0.0]\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "val_loss for each sample at the end of epoch: [0.31725031137466431, 1.0]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "val_loss for each sample at the end of epoch: [1.4835494756698608, 0.0]\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "val_loss for each sample at the end of epoch: [0.4974556565284729, 1.0]\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "val_loss for each sample at the end of epoch: [1.2606775760650635, 0.0]\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "val_loss for each sample at the end of epoch: [0.32955881953239441, 1.0]\n",
      "epoch: 9\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 0.0183 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 230ms/step - loss: 0.2439 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.0062 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 541ms/step - loss: 0.2135 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 0.4820 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.0274 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 647ms/step - loss: 0.0521 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 0.1749 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 0.7081 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 0.0339 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 0.0393 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 0.8052 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 0.1153 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0515 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 1.1083 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.0388 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 0.0923 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0098 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 0.0351 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 0.2060 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 635ms/step - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 0.0658 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.0025 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.1426 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 254ms/step - loss: 0.0120 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 0.0608 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.6711 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 0.1358 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.0374 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0219 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 652ms/step - loss: 0.0243 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.6077 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 0.0040 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 0.4339 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.0098 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 0.0341 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.0350 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 0.0459 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 0.1708 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 390ms/step - loss: 0.0118 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 0.0138 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 0.0073 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 132ms/step - loss: 0.0303 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.0204 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.0204 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.1954 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 0.0446 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 0.0134 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.6821 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 0.0015 - acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.0061 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 0.0048 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.0154 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0181 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 0.1811 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.0722 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 0.0907 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.1694 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 0.0422 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 0.0046 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 0.0586 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 0.0939 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 0.0068 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 0.2795 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 0.0450 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 0.0174 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 0.2929 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.0116 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 0.0410 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 244ms/step - loss: 0.3580 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 349ms/step - loss: 0.0168 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0051 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.0106 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0028 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 0.0422 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.0064 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 511ms/step - loss: 0.0097 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 0.4113 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0115 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 0.0765 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 420ms/step - loss: 0.0305 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 252ms/step - loss: 0.0133 - acc: 1.0000\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "val_loss for each sample at the end of epoch: [0.88194036483764648, 0.0]\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "val_loss for each sample at the end of epoch: [1.7484053373336792, 0.0]\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "val_loss for each sample at the end of epoch: [0.13717451691627502, 1.0]\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "val_loss for each sample at the end of epoch: [0.026730922982096672, 1.0]\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "val_loss for each sample at the end of epoch: [0.026989718899130821, 1.0]\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "val_loss for each sample at the end of epoch: [0.72374922037124634, 0.0]\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "val_loss for each sample at the end of epoch: [0.75622397661209106, 0.0]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "val_loss for each sample at the end of epoch: [2.6930348873138428, 0.0]\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "val_loss for each sample at the end of epoch: [0.0035967472940683365, 1.0]\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "val_loss for each sample at the end of epoch: [2.3400921821594238, 0.0]\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "val_loss for each sample at the end of epoch: [0.13130979239940643, 1.0]\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "val_loss for each sample at the end of epoch: [2.3511807918548584, 0.0]\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "val_loss for each sample at the end of epoch: [0.26095157861709595, 1.0]\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "val_loss for each sample at the end of epoch: [2.0828263759613037, 0.0]\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "val_loss for each sample at the end of epoch: [0.25045010447502136, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# Creating and fitting model\n",
    "model = create_LSTM_model(feature_size)\n",
    "epoch_histories = []\n",
    "for i in range(nb_epoch):\n",
    "    print(\"epoch: {}\".format(i))\n",
    "    train_history = []\n",
    "    for seq, label in zip(x_train, y_train):\n",
    "        loss = model.fit(np.array([seq]), label.reshape(1,2), epochs=1, batch_size=batch_size)\n",
    "        train_history.append(loss.history)\n",
    "    \n",
    "    val_history = []\n",
    "    for seq_val, label_val in zip(x_test, y_test):\n",
    "        val_loss = model.evaluate(np.array([seq_val]),label_val.reshape(1,2), batch_size=batch_size)\n",
    "        val_history.append(val_loss)\n",
    "        print(\"val_loss for each sample at the end of epoch: {}\".format(val_loss))\n",
    "    epoch_histories.append({\"train_history\": train_history, \"val_history\": val_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate_loss_acc():\n",
    "    for ep_hist in epoch_histories:\n",
    "        \n",
    "        tr_hist_list = ep_hist.get(\"train_history\")\n",
    "        train_acc = np.asscalar(np.mean(np.array([sample_history.get(\"acc\") for sample_history in tr_hist_list]), axis=0))\n",
    "        train_loss = np.asscalar(np.mean(np.array([sample_history.get(\"loss\") for sample_history in tr_hist_list]), axis=0))\n",
    "        \n",
    "        val_hist_list = ep_hist.get(\"val_history\")\n",
    "        \n",
    "        val_loss_acc = np.mean(np.array(val_hist_list), axis=0)\n",
    "        \n",
    "        yield np.hstack((np.array([train_loss, train_acc]), val_loss_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_loss_acc = np.array([s for s in accumulate_loss_acc()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f7d4f5db320>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA20AAAEyCAYAAABptTjBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nREREREQkgKloExERERERCWD/D7hv4R/0bUfUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7dcc1bf9b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (loss, acc) = plt.subplots(1,2)\n",
    "fig.set_size_inches(15,5)\n",
    "\n",
    "loss.plot(np.arange(nb_epoch), train_val_loss_acc[:,0], label=\"train_loss\")\n",
    "loss.plot(np.arange(nb_epoch), train_val_loss_acc[:,2], label=\"val_loss\")\n",
    "acc.plot(np.arange(nb_epoch), train_val_loss_acc[:,1], label=\"train_acc\")\n",
    "acc.plot(np.arange(nb_epoch), train_val_loss_acc[:,3], label=\"val_acc\")\n",
    "loss.legend()\n",
    "acc.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do prediction\n",
    "predictions = []\n",
    "for seq_test, label_test in zip(x_test, y_test):\n",
    "    pred = model.predict(np.array([seq_test]), batch_size=batch_size)\n",
    "    predictions.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.argmax(y_test, axis=1)\n",
    "y_pred = np.argmax(predictions, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7d4f311160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 200)               205600    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 206,002\n",
      "Trainable params: 206,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Test ROC Score: 0.416667\n",
      "Test RMSE Score: 0.730297\n",
      "Final Competition Score: 0.686370\n"
     ]
    }
   ],
   "source": [
    "# calculating scores\n",
    "roc_score = roc_auc_score(y_true, y_pred)\n",
    "# roc_score = roc_auc_score(np.argmax(y_test,axis=1), np.argmax(y_pred,axis=1))\n",
    "\n",
    "mse_score = mean_squared_error(y_true, y_pred)\n",
    "\n",
    "#printing and plotting model and score information\n",
    "# plot_loss(history)\n",
    "# plot_roc(y_test, y_pred)\n",
    "plot_roc(y_true, y_pred)\n",
    "# plot_accuracy(history)\n",
    "\n",
    "print(model.summary())\n",
    "print(\"Test ROC Score: %f\" % roc_score)\n",
    "print(\"Test RMSE Score: %f\" % sqrt(mse_score))\n",
    "print(\"Final Competition Score: %f\" % (1 - sqrt(mse_score) + roc_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
