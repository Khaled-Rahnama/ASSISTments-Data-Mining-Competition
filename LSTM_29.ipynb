{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changelog:\n",
    "\n",
    "* v8 Checking code and revising the debug files. Ready for debugging \n",
    "* V9 Adding resampling\n",
    "* V10 Adding tensorboard\n",
    "* V11 Reducing LSTM layer to 1 in order to have better interpretation in tensorboard\n",
    "* V12 adding actionId to data in order to debug datasets and step better + fixing the wrong column name for x_prepared + **fixing LABEL WRONG!!!!! ASSIENMENT **\n",
    "* V13 removing masking layer and padding with 0 or 9999\n",
    "* V14 removing two outputs\n",
    "* V15 returning the masking layer with mask value 99.\n",
    "    - The result is that without adding masking layer the training accuracy does not increase at all and it remains the same on nearly 50\n",
    "    - however the validation accuracy remains the same around 50 and does not increase while the validation loss is being increased as allways\n",
    "* V16 Changing the optimizer to SGD\n",
    "* V17 Changing the optimizer to rmsprop\n",
    "* V18 Increasing batch size from 1 to higher and coming back the optimizer to Adam (both roc and accuracy on train was about .9 while the accuracy of validation was 0.5\n",
    "* V19 Separating validation set from data and feed to the fit function using validation_data param\n",
    "* V20 Adding another layer of 100 unit\n",
    "* V21 Joining per stud features\n",
    "* V22 removing the sampling\n",
    "* V23 Add two output\n",
    "* V24 Assumes that we are overfitting so we are going to:\n",
    "    - Remove additional LSTM layer to simplify the model $\\checkmark$\n",
    "    - Reduce the number of units for LSTM layer $\\checkmark$ (saw that finally validation loss starts to deacrease with 20 unit and 200 seq lenght)\n",
    "    - Reduce the number of features as much as possible (should be done after dinormalizing the binary variables)\n",
    "    - Reduce the number of seq length $\\checkmark$\n",
    "* V25 Excluding binary variables and some other proportional variables from the standardization step (it seems we are improving!)\n",
    "* V26 Adding competition validation set for making prediction on un-labeled data\n",
    "* Bringing resampling back to the game!\n",
    "* V28 Adding Startify spliting\n",
    "* V29 Inversing the sequence! (it make more sence to feed the old frames first and then go forward for newer frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "var kernel = IPython.notebook.kernel;\n",
       "var thename = window.document.getElementById(\"notebook_name\").innerHTML;\n",
       "var command = \"theNotebook = \" + \"'\"+thename+\"'\";\n",
       "kernel.execute(command);"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "var kernel = IPython.notebook.kernel;\n",
    "var thename = window.document.getElementById(\"notebook_name\").innerHTML;\n",
    "var command = \"theNotebook = \" + \"'\"+thename+\"'\";\n",
    "kernel.execute(command);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.layers import LSTM, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Dropout\n",
    "from keras import optimizers\n",
    "from keras.layers import Masking\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "from Vis import plot_loss, plot_roc\n",
    "from Preprocessing import Preprocessing\n",
    "from Preprocessing import Cols\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "# pandas.set_option('max_columns',10)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepared dataset contains: per_stud_dataset + per_action_dataset_summ\n"
     ]
    }
   ],
   "source": [
    "pre = Preprocessing()\n",
    "_,_ = pre.load_data(time_gap=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "per_action_dataset = pre.per_action_dataset\n",
    "per_action_dataset.index = per_action_dataset.ITEST_id\n",
    "\n",
    "per_action_dataset = pre.per_action_dataset.drop(['ITEST_id', 'assistmentId', 'problemId', 'assignmentId'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "per_stud_dataset = pre.per_stud_dataset.drop(\"isSTEM\", axis=1)\n",
    "per_stud_dataset.index = per_stud_dataset.ITEST_id\n",
    "per_stud_dataset = per_stud_dataset.drop(\"ITEST_id\", axis=1)\n",
    "dataset = per_stud_dataset.join(per_action_dataset)\n",
    "dataset.index = pd.MultiIndex.from_arrays([dataset.index.values, dataset.actionId.values], names=[\"ITEST_id\", \"actionId\"])\n",
    "dataset = dataset.drop(\"actionId\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_dataset = pre.test_dataset.drop(\"ITEST_id\", axis=1)\n",
    "test_dataset.index = pre.test_dataset.ITEST_id\n",
    "\n",
    "shared_ids_with_train = test_dataset.index.intersection(dataset.index.get_level_values(0).unique().values)\n",
    "pure_testset = test_dataset.drop(shared_ids_with_train)\n",
    "raw_dataset = pre.raw_dataset.drop(Cols.excluded_cols + [\"ITEST_id\"], axis=1)\n",
    "raw_dataset.index = pre.raw_dataset.ITEST_id\n",
    "\n",
    "included_cols = list(set(Cols.per_action_cols + Cols.per_stud_cols).difference(set(['isSTEM', 'assistmentId', 'problemId', 'assignmentId'] + Cols.excluded_cols + Cols.per_stud_cols_cat + Cols.per_action_cols_cat)))\n",
    "pure_testset_perActionAndStud = pure_testset.join(raw_dataset)[included_cols]\n",
    "pure_testset_perActionAndStud.index = pd.MultiIndex.from_arrays([pure_testset_perActionAndStud.index.values,pure_testset_perActionAndStud.actionId.values ], names=[\"ITEST_id\", \"actionId\"])\n",
    "pure_testset_perActionAndStud = pure_testset_perActionAndStud.drop(\"actionId\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.concat([dataset, pure_testset_perActionAndStud])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining sequence length (or number of time-steps) for each student and batch-size\n",
    "max_length_seq = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show the seq of a sample student (id =9)\n",
    "#per_action_dataset[per_action_dataset.index.get_level_values(0) ==9].head()\n",
    "# per_action_dataset.to_csv(\"Debug/1-per_action_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# truncating seq of each student and only considering his last 'max_length_seq' actions based on startTime feature\n",
    "truncated_input = dataset.sort_values(\"startTime\",ascending=True).groupby('ITEST_id').tail(max_length_seq)\n",
    "# truncated_input.to_csv(\"Debug/2-truncated_input.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for excluding binary features from standardization process\n",
    "binary_cols = ['AveKnow', 'AveCarelessness', 'correct', 'original', 'hint', 'scaffold', 'bottomHint', 'frIsHelpRequest', 'stlHintUsed', 'frWorkingInSchool',\n",
    "               'responseIsFillIn', 'responseIsChosen', 'endsWithScaffolding', 'endsWithAutoScaffolding', 'frIsHelpRequestScaffolding', 'timeGreater5Secprev2wrong', 'helpAccessUnder2Sec', 'timeGreater10SecAndNextActionRight', 'timeOver80', 'manywrong']\n",
    "should_not_normalize_cols = ['RES_BORED', 'RES_CONCENTRATING', 'RES_CONFUSED', 'RES_FRUSTRATED', 'RES_OFFTASK', 'RES_GAMING']\n",
    "# also for 'AveCorrect', AveResBored, AveResEngcon, AveResConf, AveResFrust, AveResOfftask, AveResGaming, , Ln-1, Ln, \n",
    "should_not_normalized = truncated_input[should_not_normalize_cols + binary_cols]\n",
    "should_normalized = truncated_input.drop(should_not_normalize_cols + binary_cols, axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# scaling dataset\n",
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "scaled_data = scaler.fit_transform(should_normalized)\n",
    "scaled_data = DataFrame(scaled_data, index=should_normalized.index, columns=should_normalized.columns)\n",
    "X_scaled = scaled_data.join(should_not_normalized)\n",
    "\n",
    "# X_scaled[X_scaled.index.get_level_values(0) ==9].head()\n",
    "# X_scaled.to_csv(\"Debug/3-X_scaled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()  # create figure & 1 axis\n",
    "# X_scaled.boxplot(X_scaled.columns.difference([\"startTime\", 'endTime']).values.tolist(),ax=ax)\n",
    "# fig.set_size_inches(100,100)\n",
    "# fig.savefig('X_scaled.png')   # save the figure to file\n",
    "# plt.close(fig) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# batch size should be one as we are facing with a problem with several independent sequences of student actions \n",
    "batch_size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# padding sequences to have a same length\n",
    "X_padded = []\n",
    "for stud_id, stud_seq in X_scaled.groupby('ITEST_id'):\n",
    "    X_padded.append(stud_seq)\n",
    "# changing truncating from post to pre after inversing the seq. Also it makes more sense to change the padding from post to pre!    \n",
    "X_padded = sequence.pad_sequences(X_padded, max_length_seq, dtype='float32', padding=\"pre\", truncating=\"pre\", value=99.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# getting a sense about padded dataset\n",
    "ITEST_ix = pd.Index(np.repeat(sorted(X_scaled.index.get_level_values(0).unique()), max_length_seq), name='ITEST_id')\n",
    "new_seq_ix = pd.Index(list(range(0,max_length_seq))* X_padded.shape[0], name=\"seq_ix\")\n",
    "paddedData = DataFrame(X_padded.reshape(X_padded.shape[0]*X_padded.shape[1], -1), index=[ITEST_ix, new_seq_ix], columns=X_scaled.columns)\n",
    "# paddedData.to_csv(\"Debug/4-paddedData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"MAX_ROWS\", 10)\n",
    "pd.set_option(\"MAX_COLUMNS\", 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>attemptCount</th>\n",
       "      <th>consecutiveErrorsInRow</th>\n",
       "      <th>endTime</th>\n",
       "      <th>...</th>\n",
       "      <th>timeGreater10SecAndNextActionRight</th>\n",
       "      <th>timeOver80</th>\n",
       "      <th>manywrong</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ITEST_id</th>\n",
       "      <th>seq_ix</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"11\" valign=\"top\">1667</th>\n",
       "      <th>0</th>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>-0.830508</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.498875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.498872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.964286</td>\n",
       "      <td>-0.498871</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>-0.966102</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.498869</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.498868</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 attemptCount  consecutiveErrorsInRow    endTime    ...      \\\n",
       "ITEST_id seq_ix                                                     ...       \n",
       "1667     0          99.000000               99.000000  99.000000    ...       \n",
       "         1          99.000000               99.000000  99.000000    ...       \n",
       "         2          99.000000               99.000000  99.000000    ...       \n",
       "         3          99.000000               99.000000  99.000000    ...       \n",
       "         4          99.000000               99.000000  99.000000    ...       \n",
       "...                       ...                     ...        ...    ...       \n",
       "         195        -0.830508               -1.000000  -0.498875    ...       \n",
       "         196        -1.000000               -1.000000  -0.498872    ...       \n",
       "         197        -1.000000               -0.964286  -0.498871    ...       \n",
       "         198        -0.966102               -1.000000  -0.498869    ...       \n",
       "         199        -1.000000               -1.000000  -0.498868    ...       \n",
       "\n",
       "                 timeGreater10SecAndNextActionRight  timeOver80  manywrong  \n",
       "ITEST_id seq_ix                                                             \n",
       "1667     0                                     99.0        99.0       99.0  \n",
       "         1                                     99.0        99.0       99.0  \n",
       "         2                                     99.0        99.0       99.0  \n",
       "         3                                     99.0        99.0       99.0  \n",
       "         4                                     99.0        99.0       99.0  \n",
       "...                                             ...         ...        ...  \n",
       "         195                                    0.0         0.0        1.0  \n",
       "         196                                    0.0         1.0        1.0  \n",
       "         197                                    0.0         0.0        1.0  \n",
       "         198                                    1.0         1.0        1.0  \n",
       "         199                                    0.0         0.0        1.0  \n",
       "\n",
       "[200 rows x 49 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paddedData[paddedData.index.get_level_values(0) == 1667].drop([\"MCAS\", \"NumActions\", \"SchoolId\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creating label dataset with index as ITEST_id values\n",
    "label_dataset = pre.label_dataset[['ITEST_id', 'isSTEM']]\n",
    "label_dataset.index = label_dataset.ITEST_id\n",
    "label_dataset = label_dataset.drop(\"ITEST_id\", axis = 1)\n",
    "#label_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# separating unlabeled data from labeled data and preparing competition data for prediction\n",
    "X_competition = paddedData.loc[pure_testset.index.unique().values, :]\n",
    "paddedData = paddedData.drop(pure_testset.index.unique().values)\n",
    "X_competition_arr = X_competition.values.reshape(-1, max_length_seq, X_competition.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# join label with dataset to make sure we have a consistent dataset\n",
    "# (of course before training starts the labels column will be droped from data)\n",
    "X_with_label = paddedData.join(label_dataset)\n",
    "#print(X_with_label.shape)\n",
    "# X_with_label.to_csv(\"Debug/5-X_with_label.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# balancing data by drawing a set of samples without replacement from the majority calss with the length of minority class\n",
    "from sklearn.utils import resample\n",
    "\n",
    "df_majority = X_with_label[X_with_label['isSTEM'] == 0]\n",
    "df_minority = X_with_label[X_with_label['isSTEM'] == 1]\n",
    "\n",
    "minority_len = len(df_minority.index.get_level_values(0).unique())\n",
    "majority_len = len(df_majority.index.get_level_values(0).unique())\n",
    "\n",
    "sampled_df_majority = df_majority.loc[resample(df_majority.index.get_level_values(0).unique(), n_samples=minority_len, replace=False).values]\n",
    "\n",
    "balanced_X = pd.concat([df_minority, sampled_df_majority])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# y_prepared = X_with_label[~X_with_label.index.get_level_values(0).duplicated()]['isSTEM'].values\n",
    "# X_prepared = X_with_label.drop(\"isSTEM\", axis=1)\n",
    "# X_prepared_columns = X_prepared.columns\n",
    "# X_prepared = X_prepared.values.reshape((-1, max_length_seq, X_prepared.shape[1]))\n",
    "# DataFrame(X_prepared.reshape(X_prepared.shape[0] * X_prepared.shape[1], X_prepared.shape[2]), index=X_with_label.index, columns=X_prepared_columns).to_csv(\"Debug/6-X_prepared.csv\")\n",
    "\n",
    "#for balanced dataset only \n",
    "y_prepared = balanced_X[~balanced_X.index.get_level_values(0).duplicated()]['isSTEM'].values\n",
    "X_prepared = balanced_X.drop(\"isSTEM\", axis=1)\n",
    "X_prepared_columns = X_prepared.columns\n",
    "X_prepared = X_prepared.values.reshape((-1, max_length_seq, X_prepared.shape[1]))\n",
    "\n",
    "# DataFrame(X_prepared.reshape(X_prepared.shape[0] * X_prepared.shape[1], X_prepared.shape[2]), index=balanced_X.index, columns=X_prepared_columns).to_csv(\"Debug/6-X_prepared.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_LSTM_model(max_length_seq, feature_size):\n",
    "    model = Sequential()\n",
    "    model.add(Masking(mask_value=99., input_shape=(max_length_seq, feature_size)))\n",
    "    model.add(LSTM(300))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# splitting train/test data \n",
    "X_train, X_test, y_train,y_test = train_test_split(X_prepared,y_prepared, stratify=y_prepared, train_size=.77)\n",
    "enc = OneHotEncoder()\n",
    "y_train = enc.fit_transform(y_train.reshape(-1,1)).toarray()\n",
    "y_test = enc.fit_transform(y_test.reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# xx_train = DataFrame(X_train.reshape(X_train.shape[0] * X_train.shape[1], X_train.shape[2]))\n",
    "\n",
    "# xx_train['label'] = np.repeat(np.argmax(y_train, axis=1), 500)\n",
    "\n",
    "# xx_train.to_csv(\"xx_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard, EarlyStopping\n",
    "tensorboard_callback = TensorBoard(log_dir='./logs', histogram_freq=1, batch_size=batch_size, write_graph=True, write_grads=True, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', min_delta=0.005, patience=10, verbose=10, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 180 samples, validate on 54 samples\n",
      "Epoch 1/20\n",
      "180/180 [==============================] - 38s 208ms/step - loss: 0.7510 - acc: 0.4611 - val_loss: 0.7035 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      "180/180 [==============================] - 36s 200ms/step - loss: 0.6750 - acc: 0.6556 - val_loss: 0.7473 - val_acc: 0.4815\n",
      "Epoch 3/20\n",
      "180/180 [==============================] - 36s 200ms/step - loss: 0.6778 - acc: 0.5722 - val_loss: 0.7074 - val_acc: 0.4815\n",
      "Epoch 4/20\n",
      "180/180 [==============================] - 36s 199ms/step - loss: 0.6658 - acc: 0.6000 - val_loss: 0.7280 - val_acc: 0.4815\n",
      "Epoch 5/20\n",
      "180/180 [==============================] - 36s 198ms/step - loss: 0.6616 - acc: 0.6500 - val_loss: 0.7365 - val_acc: 0.4444\n",
      "Epoch 6/20\n",
      "180/180 [==============================] - 36s 199ms/step - loss: 0.6154 - acc: 0.6833 - val_loss: 0.8231 - val_acc: 0.4630\n",
      "Epoch 7/20\n",
      "180/180 [==============================] - 36s 198ms/step - loss: 0.9299 - acc: 0.5500 - val_loss: 0.7157 - val_acc: 0.4444\n",
      "Epoch 8/20\n",
      "180/180 [==============================] - 36s 200ms/step - loss: 0.7330 - acc: 0.5167 - val_loss: 0.6874 - val_acc: 0.5370\n",
      "Epoch 9/20\n",
      "180/180 [==============================] - 35s 193ms/step - loss: 0.7068 - acc: 0.5000 - val_loss: 0.6741 - val_acc: 0.5926\n",
      "Epoch 10/20\n",
      "180/180 [==============================] - 34s 190ms/step - loss: 0.7042 - acc: 0.5278 - val_loss: 0.6678 - val_acc: 0.6111\n",
      "Epoch 11/20\n",
      "180/180 [==============================] - 34s 191ms/step - loss: 0.6842 - acc: 0.5500 - val_loss: 0.6897 - val_acc: 0.5926\n",
      "Epoch 12/20\n",
      "180/180 [==============================] - 38s 214ms/step - loss: 0.6606 - acc: 0.5944 - val_loss: 0.6992 - val_acc: 0.5370\n",
      "Epoch 13/20\n",
      "180/180 [==============================] - 38s 208ms/step - loss: 0.6531 - acc: 0.6333 - val_loss: 0.7317 - val_acc: 0.5000\n",
      "Epoch 14/20\n",
      "180/180 [==============================] - 36s 200ms/step - loss: 0.6524 - acc: 0.6111 - val_loss: 0.7563 - val_acc: 0.5185\n",
      "Epoch 15/20\n",
      "180/180 [==============================] - 36s 201ms/step - loss: 0.6462 - acc: 0.6333 - val_loss: 0.7243 - val_acc: 0.5185\n",
      "Epoch 16/20\n",
      "180/180 [==============================] - 36s 202ms/step - loss: 0.6770 - acc: 0.6056 - val_loss: 0.7188 - val_acc: 0.5556\n",
      "Epoch 17/20\n",
      "180/180 [==============================] - 36s 199ms/step - loss: 0.6659 - acc: 0.6000 - val_loss: 0.7603 - val_acc: 0.5185\n",
      "Epoch 18/20\n",
      "180/180 [==============================] - 36s 199ms/step - loss: 0.6544 - acc: 0.6000 - val_loss: 0.7173 - val_acc: 0.4630\n",
      "Epoch 19/20\n",
      "180/180 [==============================] - 36s 201ms/step - loss: 0.6463 - acc: 0.6333 - val_loss: 0.7469 - val_acc: 0.5556\n",
      "Epoch 20/20\n",
      "180/180 [==============================] - 38s 208ms/step - loss: 0.6401 - acc: 0.6278 - val_loss: 0.7905 - val_acc: 0.5370\n",
      "Epoch 00020: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = create_LSTM_model(max_length_seq, X_train.shape[2])\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=batch_size, validation_data= (X_test,y_test), callbacks=[tensorboard_callback, early_stopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# y_pred_competition = model.predict(X_competition_arr, batch_size=batch_size)\n",
    "\n",
    "# result_index = X_competition.reset_index(level=1, drop=True).index.unique()\n",
    "\n",
    "# argmax_preds = [np.argmax(predicted_label) for predicted_label in y_pred_competition]\n",
    "\n",
    "# result_df = DataFrame(argmax_preds, index=pd.Index(result_index, name='ITEST_id'), columns=['isSTEM'])\n",
    "\n",
    "# final_output = pd.concat([result_df, label_dataset.loc[shared_ids_with_train.values]]).sort_index()\n",
    "# final_output.to_csv(\"submition_1_{}.csv\".format(theNotebook))\n",
    "# final_output.isSTEM.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
