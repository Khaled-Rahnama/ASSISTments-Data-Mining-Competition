{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changelog:\n",
    "\n",
    "* v8 Checking code and revising the debug files. Ready for debugging \n",
    "* V9 Adding resampling\n",
    "* V10 Adding tensorboard\n",
    "* V11 Reducing LSTM layer to 1 in order to have better interpretation in tensorboard\n",
    "* V12 adding actionId to data in order to debug datasets and step better + fixing the wrong column name for x_prepared + **fixing LABEL WRONG!!!!! ASSIGENMENT **\n",
    "* V13 removing masking layer and padding with 0 or 9999\n",
    "* V14 removing two outputs\n",
    "* V15 returning the masking layer with mask value 99.\n",
    "    - The result is that without adding masking layer the training accuracy does not increase at all and it remains the same on nearly 50\n",
    "    - however the validation accuracy remains the same around 50 and does not increase while the validation loss is being increased as allways\n",
    "* V16 Changing the optimizer to SGD\n",
    "* V17 Changing the optimizer to rmsprop\n",
    "* V18 Increasing batch size from 1 to higher and coming back the optimizer to Adam (both roc and accuracy on train was about .9 while the accuracy of validation was 0.5\n",
    "* V19 Separating validation set from data and feed to the fit function using validation_data param\n",
    "* V20 Adding another layer of 100 unit\n",
    "* V21 Joining per stud features\n",
    "* V22 removing the sampling\n",
    "* V23 Add two output\n",
    "* V24 Assumes that we are overfitting so we are going to:\n",
    "    - Remove additional LSTM layer to simplify the model $\\checkmark$\n",
    "    - Reduce the number of units for LSTM layer $\\checkmark$ (saw that finally validation loss starts to deacrease with 20 unit and 200 seq lenght)\n",
    "    - Reduce the number of features as much as possible (should be done after dinormalizing the binary variables)\n",
    "    - Reduce the number of seq length $\\checkmark$\n",
    "* V25 Excluding binary variables and some other proportional variables from the standardization step (it seems we are improving!)\n",
    "* V26 Adding competition validation set for making prediction on un-labeled data\n",
    "* Bringing resampling back to the game!\n",
    "* V28 Adding Startify spliting\n",
    "* V29 Inversing the sequence! (it make more sence to feed the old frames first and then go forward for newer frames)\n",
    "* V30 A revolutionary update: removing the padding and updating the weights with variable lenght sequences!\n",
    "* V31 Testing various model architecture and layers with different number of units\n",
    "    - V31.1 Disabling resampling\n",
    "* V32 Exclude students with large number of actions e.g. greater than 2000\n",
    "* V33 Encode categorical features + **USING MinMaxScaler**\n",
    "* V34 Removing outliers for MinMax feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import TensorBoard, EarlyStopping\n",
    "from keras.preprocessing import sequence\n",
    "from keras import optimizers\n",
    "\n",
    "from Vis import plot_loss, plot_roc, plot_accuracy\n",
    "from Preprocessing import Preprocessing, Cols\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler, StandardScaler, MinMaxScaler\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from pandas import DataFrame, Series\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from math import sqrt\n",
    "from itertools import chain\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "# pandas.set_option('max_columns',10)\n",
    "\n",
    "import os\n",
    "import glob\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loading datasets and labels\n",
    "data_files = glob.glob(os.path.join(\"Dataset\", \"student_log_*.csv\"))\n",
    "raw_dataset = pd.concat((pd.read_csv(f, index_col=[\"ITEST_id\"]) for f in data_files))\n",
    "\n",
    "# dataset = raw_dataset.drop(Cols.excluded_cols + Cols.cat_cols, axis=1)\n",
    "dataset = raw_dataset.drop(Cols.excluded_cols, axis=1)\n",
    "\n",
    "labels = DataFrame.from_csv(\"Dataset/training_label.csv\")\n",
    "valid_test_label_dataset = DataFrame.from_csv(\"Dataset/validation_test_label.csv\")\n",
    "\n",
    "unlabels = valid_test_label_dataset.drop(list(labels.index.intersection(valid_test_label_dataset.index)))\n",
    "labels_unlabels = pd.concat([labels,unlabels])\n",
    "\n",
    "dwlu = dataset.join(labels_unlabels, how=\"inner\") # dwlu = dataset_with_labels_unlabels\n",
    "\n",
    "dwlu.index = pd.MultiIndex.from_arrays([dwlu.index, dwlu.actionId])\n",
    "\n",
    "dwlu = dwlu.drop(\"actionId\", axis =1)\n",
    "dwlu = dwlu.sort_values(\"startTime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Columns: {'skill': 93} {'problemType': 16} {'SY ASSISTments Usage': 2} {'MCAS': 51} {'SchoolId': 4}\n"
     ]
    }
   ],
   "source": [
    "# Converting category variables to dummy variables\n",
    "cat_cols = ['skill', 'problemType', 'SY ASSISTments Usage', 'MCAS', 'SchoolId']\n",
    "\n",
    "new_cols = [{cc: len(dwlu[cc].unique())} for cc in cat_cols]\n",
    "print(\"New Columns:\" , *new_cols)\n",
    "dwlu = pd.get_dummies(dwlu, columns=cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 students are removed!\n"
     ]
    }
   ],
   "source": [
    "# Excluding students with large number of actions (does not matter whether they are isSTEM=1 or not but does matter if they are isSTEM=NAN)\n",
    "isLarge = (dwlu.groupby(\"ITEST_id\").size() > 2000)\n",
    "largeStuds_ids = isLarge[isLarge == True].index.values\n",
    "largeStuds_ids_with_label = [l for l in largeStuds_ids if l not in unlabels.index.values]\n",
    "\n",
    "print(\"%d students are removed!\" % len(largeStuds_ids_with_label))\n",
    "dwlu = dwlu.drop(largeStuds_ids_with_label, level=0)\n",
    "\n",
    "# no unlabeled data should be removed\n",
    "assert(len(dwlu[dwlu.isSTEM.isnull()].index.get_level_values(0).unique()) == len(unlabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling data in order to balance class labels\n",
    "from sklearn.utils import resample\n",
    "\n",
    "df_majority = dwlu[dwlu['isSTEM'] == 0]\n",
    "df_minority = dwlu[dwlu['isSTEM'] == 1]\n",
    "df_unlabeled = dwlu[dwlu['isSTEM'].isnull()]\n",
    "\n",
    "minority_len = len(df_minority.index.get_level_values(0).unique())\n",
    "\n",
    "majority_ids = df_majority.index.get_level_values(0).unique()\n",
    "sample_majority_ids = resample(majority_ids , n_samples=minority_len, replace=False).values\n",
    "\n",
    "sampled_df_majority = df_majority.sort_index(level=0).loc[sample_majority_ids, :]\n",
    "\n",
    "dwlu = pd.concat([df_minority, sampled_df_majority, df_unlabeled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Listing all dummy variables\n",
    "all_dummy_cols = [[col for col in dwlu.columns if cat+\"_\" in col] for cat in cat_cols ]\n",
    "all_dummy_cols = list(chain.from_iterable(all_dummy_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying columns that should be normalized\n",
    "binary_cols = ['AveKnow', 'AveCarelessness', 'correct', 'original', 'hint', 'scaffold', 'bottomHint', 'frIsHelpRequest', 'stlHintUsed', 'frWorkingInSchool',\n",
    "               'responseIsFillIn', 'responseIsChosen', 'endsWithScaffolding', 'endsWithAutoScaffolding', 'frIsHelpRequestScaffolding', 'timeGreater5Secprev2wrong', 'helpAccessUnder2Sec', 'timeGreater10SecAndNextActionRight', 'timeOver80', 'manywrong']\n",
    "res_cols = ['RES_BORED', 'RES_CONCENTRATING', 'RES_CONFUSED', 'RES_FRUSTRATED', 'RES_OFFTASK', 'RES_GAMING']\n",
    "should_not_normalize_cols = ['isSTEM'] + res_cols + binary_cols + all_dummy_cols\n",
    "# also for 'AveCorrect', 'AveResBored', 'AveResEngcon', 'AveResConf', 'AveResFrust', 'AveResOfftask', 'AveResGaming', 'Ln-1', 'Ln', \n",
    "should_not_normalized = dwlu[should_not_normalize_cols]\n",
    "should_normalized = dwlu.drop(should_not_normalize_cols, axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling necessary columns\n",
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "scaled_data = scaler.fit_transform(should_normalized)\n",
    "scaled_data = DataFrame(scaled_data, index=should_normalized.index, columns=should_normalized.columns)\n",
    "scaled_dwlu = scaled_data.join(should_not_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing some features to avoid overfitting\n",
    "# unimportant_features = scaled_dwlu.columns.difference(Cols.paper_suggested_cols).values.tolist()\n",
    "# unimportant_features.remove(\"isSTEM\")\n",
    "# scaled_dwlu = scaled_dwlu.drop(unimportant_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection!\n",
    "selected_features = ['isSTEM', 'AveKnow', 'AveCarelessness', 'hint', 'timeGreater10SecAndNextActionRight', 'bottomHint', 'correct', 'past8BottomOut','manywrong', 'hintCount','hintTotal']\n",
    "scaled_dwlu = scaled_dwlu[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building learning datasets/labels and competition preditcion dataset\n",
    "x_competition = scaled_dwlu.loc[unlabels.index.values, :].drop(\"isSTEM\", axis=1)\n",
    "x = scaled_dwlu.loc[labels.index.values, :].drop(\"isSTEM\", axis=1)\n",
    "y = scaled_dwlu.loc[labels.index.values, :][['isSTEM']].reset_index(level=1, drop=True)\n",
    "y = y[~y.index.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting X from a 2D dataframe to a 3D list of varibale-lenght sequecens\n",
    "x = np.array([stud_seq.values for _, stud_seq in x.groupby(\"ITEST_id\")])\n",
    "y = y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting train/test data \n",
    "# TODO test Startify\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding train/test labels\n",
    "enc = OneHotEncoder()\n",
    "y_train = enc.fit_transform(y_train.reshape(-1,1)).toarray()\n",
    "y_test = enc.fit_transform(y_test.reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining model configs.\n",
    "batch_size = 1 # sequences are independent and the network weights should be updated after each sequence\n",
    "feature_size = x_train[0].shape[1] # num. of features extracted from first training sample\n",
    "timestep_size = None # here we are using variable-length sequences so there is no fixed timestep size\n",
    "nb_epoch = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating necessary callbacks for early stopping and Tensorboard visualization\n",
    "tensorboard_callback = TensorBoard(log_dir='./logs', histogram_freq=1, batch_size=batch_size, write_graph=True, write_grads=True, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', min_delta=0.005, patience=5, verbose=10, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a variable length model!\n",
    "def create_LSTM_model(feature_size):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(32, input_shape=(None, feature_size)))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 913ms/step - loss: 0.8483 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 0.6137 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 533ms/step - loss: 0.6746 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 434ms/step - loss: 0.5532 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.5774 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 608ms/step - loss: 0.5654 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 570ms/step - loss: 0.9415 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 708ms/step - loss: 0.9356 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.5803 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 234ms/step - loss: 0.8253 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 772ms/step - loss: 0.5085 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 0.5571 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 555ms/step - loss: 0.8451 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.9480 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 0.5421 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.5660 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 530ms/step - loss: 1.0023 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 0.5319 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 582ms/step - loss: 0.9476 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 0.5359 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.9085 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 0.4998 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 0.9370 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.5028 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.5204 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 353ms/step - loss: 0.8310 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.4866 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.9366 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 391ms/step - loss: 0.4849 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 968ms/step - loss: 0.8804 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 0.5693 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 806ms/step - loss: 0.5014 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 398ms/step - loss: 0.8274 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.5620 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.8823 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 0.5144 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 0.4878 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 393ms/step - loss: 0.5189 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.8823 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.5562 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 408ms/step - loss: 0.5367 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 246ms/step - loss: 0.9631 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 518ms/step - loss: 0.5708 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.5687 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.5115 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 402ms/step - loss: 0.9289 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 556ms/step - loss: 0.8662 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 244ms/step - loss: 0.8769 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 242ms/step - loss: 0.5158 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 376ms/step - loss: 0.8700 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 529ms/step - loss: 0.8696 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.8397 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.5966 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 606ms/step - loss: 0.5009 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.7885 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 541ms/step - loss: 0.7747 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.8489 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.5596 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 0.6185 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.8957 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.6628 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.7620 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 803ms/step - loss: 0.7435 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 0.7053 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 226ms/step - loss: 0.6529 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 450ms/step - loss: 0.7039 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.7402 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.7683 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 467ms/step - loss: 0.6836 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.6819 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.6362 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 534ms/step - loss: 0.7226 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 0.7022 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 0.7064 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.7359 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 641ms/step - loss: 0.7232 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.7520 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.6906 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.6676 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.5978 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 980ms/step - loss: 0.6950 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 427ms/step - loss: 0.6917 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 881ms/step - loss: 0.7167 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.8063 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 0.6912 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 0.6609 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 0.6082 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.5921 - acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.8807 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.5655 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7519 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 0.8746 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.6107 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6952 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.8845 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.7121 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.5029 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 602ms/step - loss: 0.6102 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5013 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.4936 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.9049 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 563ms/step - loss: 0.4485 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.9526 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.4239 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 427ms/step - loss: 0.4937 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.4293 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 364ms/step - loss: 0.3893 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.3762 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.1748 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.9702 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.3692 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 1.1023 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 0.3530 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 1.1319 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 809ms/step - loss: 0.3050 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 1.0583 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 0.9076 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 524ms/step - loss: 0.4181 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 0.3438 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.9389 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.8493 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.9624 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 0.7925 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.8712 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 0.8005 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 1.0231 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.6627 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 0.7400 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 823ms/step - loss: 0.7774 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 0.7827 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 368ms/step - loss: 0.8247 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 0.8428 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 532ms/step - loss: 0.6151 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 722ms/step - loss: 0.6014 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 416ms/step - loss: 0.6330 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.8411 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 937ms/step - loss: 0.7018 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 0.5066 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 462ms/step - loss: 0.5297 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.6873 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.7347 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 354ms/step - loss: 0.8344 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 324ms/step - loss: 0.5849 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.6423 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 463ms/step - loss: 0.6346 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.6516 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 533ms/step - loss: 0.8904 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 457ms/step - loss: 0.7350 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5379 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.8827 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.5260 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8481 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.6681 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5650 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 0.7327 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.8427 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 388ms/step - loss: 0.5304 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 450ms/step - loss: 0.7881 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.6855 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 463ms/step - loss: 0.7655 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.8723 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 559ms/step - loss: 0.4358 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.7563 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 1.0395 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 0.4276 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 0.7350 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.7644 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 0.5381 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.7459 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.5196 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 354ms/step - loss: 0.9336 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.4513 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 0.7574 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 0.5682 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.5352 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.4969 - acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.7557 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.7762 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 603ms/step - loss: 0.5868 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.6184 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.8145 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 229ms/step - loss: 0.8015 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.8419 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 463ms/step - loss: 0.7865 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 0.9462 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 240ms/step - loss: 0.7520 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 0.8150 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.5162 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 477ms/step - loss: 0.7159 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.7845 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 0.6642 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 494ms/step - loss: 0.6565 - acc: 1.0000\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "val_loss for each sample at the end of epoch: [0.7583574652671814, 0.0]\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "val_loss for each sample at the end of epoch: [0.53743976354598999, 1.0]\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "val_loss for each sample at the end of epoch: [0.77186089754104614, 0.0]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "val_loss for each sample at the end of epoch: [0.49978089332580566, 1.0]\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "val_loss for each sample at the end of epoch: [0.56099808216094971, 1.0]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "val_loss for each sample at the end of epoch: [0.76649177074432373, 0.0]\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "val_loss for each sample at the end of epoch: [0.8016623854637146, 0.0]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "val_loss for each sample at the end of epoch: [0.57829976081848145, 1.0]\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "val_loss for each sample at the end of epoch: [0.56342905759811401, 1.0]\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "val_loss for each sample at the end of epoch: [0.76469236612319946, 0.0]\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "val_loss for each sample at the end of epoch: [0.83897244930267334, 0.0]\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "val_loss for each sample at the end of epoch: [0.62012112140655518, 1.0]\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "val_loss for each sample at the end of epoch: [0.65691483020782471, 1.0]\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "val_loss for each sample at the end of epoch: [0.78920543193817139, 0.0]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "val_loss for each sample at the end of epoch: [0.63713651895523071, 1.0]\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "val_loss for each sample at the end of epoch: [0.54505300521850586, 1.0]\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "val_loss for each sample at the end of epoch: [0.88695693016052246, 0.0]\n",
      "1/1 [==============================] - 0s 197ms/step\n",
      "val_loss for each sample at the end of epoch: [0.89466220140457153, 0.0]\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "val_loss for each sample at the end of epoch: [0.5280689001083374, 1.0]\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "val_loss for each sample at the end of epoch: [0.66763997077941895, 1.0]\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "val_loss for each sample at the end of epoch: [0.53310883045196533, 1.0]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "val_loss for each sample at the end of epoch: [0.54317176342010498, 1.0]\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "val_loss for each sample at the end of epoch: [0.50187069177627563, 1.0]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "val_loss for each sample at the end of epoch: [0.7060394287109375, 0.0]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "val_loss for each sample at the end of epoch: [0.71461158990859985, 0.0]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "val_loss for each sample at the end of epoch: [0.81196391582489014, 0.0]\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "val_loss for each sample at the end of epoch: [0.60221964120864868, 1.0]\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "val_loss for each sample at the end of epoch: [0.73735445737838745, 0.0]\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "val_loss for each sample at the end of epoch: [0.73907077312469482, 0.0]\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "val_loss for each sample at the end of epoch: [0.78156197071075439, 0.0]\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "val_loss for each sample at the end of epoch: [0.84007728099822998, 0.0]\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "val_loss for each sample at the end of epoch: [0.68228840827941895, 1.0]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "val_loss for each sample at the end of epoch: [0.89111912250518799, 0.0]\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "val_loss for each sample at the end of epoch: [0.43628722429275513, 1.0]\n",
      "epoch: 1\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.7473 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 0.5857 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 527ms/step - loss: 0.6361 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 434ms/step - loss: 0.7648 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.8096 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 606ms/step - loss: 0.4813 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 568ms/step - loss: 1.0186 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 710ms/step - loss: 0.5937 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 0.7224 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 0.6176 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 762ms/step - loss: 0.4727 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 0.6468 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 556ms/step - loss: 0.6842 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8040 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 0.5470 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.7727 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 531ms/step - loss: 0.6667 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 0.5501 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 584ms/step - loss: 0.7755 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 0.7217 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7818 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 0.4975 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 1.0067 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.5992 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.5707 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 353ms/step - loss: 0.7909 - acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 163ms/step - loss: 0.6475 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 1.0372 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 389ms/step - loss: 0.4174 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 974ms/step - loss: 0.8585 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 0.6786 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 804ms/step - loss: 0.4401 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 398ms/step - loss: 0.7870 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.5719 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.9173 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 0.5790 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 0.4248 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 390ms/step - loss: 0.5176 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.9080 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.5680 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 412ms/step - loss: 0.4632 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 0.7638 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 525ms/step - loss: 0.6060 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.5793 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.5159 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 397ms/step - loss: 1.0287 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 550ms/step - loss: 0.8131 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.8968 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 242ms/step - loss: 0.4845 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 0.9218 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 536ms/step - loss: 0.7553 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.8690 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.5797 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 599ms/step - loss: 0.4458 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 151ms/step - loss: 0.8202 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 549ms/step - loss: 0.7687 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 0.9984 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 0.4664 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 0.7526 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.8653 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.6017 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 0.6375 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 799ms/step - loss: 0.5918 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 0.6319 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 0.5484 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 446ms/step - loss: 0.8015 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.7967 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.7730 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 470ms/step - loss: 0.5826 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.6900 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.5260 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 531ms/step - loss: 0.8637 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 0.7021 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 0.7558 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.6538 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 632ms/step - loss: 0.5702 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.6697 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.5717 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.8302 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5986 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 985ms/step - loss: 0.7823 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 424ms/step - loss: 0.5467 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 883ms/step - loss: 0.5836 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.9663 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 0.5909 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 0.5260 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 0.5673 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.5493 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.8992 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.5179 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8696 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.8992 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.7127 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8614 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.9356 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.8809 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.5305 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 604ms/step - loss: 0.7161 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.4308 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.6010 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.7057 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 558ms/step - loss: 0.5026 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.7848 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.4371 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 423ms/step - loss: 0.6517 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.3576 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 361ms/step - loss: 0.3595 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.4322 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.0170 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.7311 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.4859 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.8474 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 0.3213 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 1.1549 - acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 805ms/step - loss: 0.2883 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.8237 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 0.7219 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 526ms/step - loss: 0.4811 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 244ms/step - loss: 0.2979 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.8079 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 0.6921 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.8449 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 0.7220 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 120ms/step - loss: 0.8256 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 0.7458 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 129ms/step - loss: 1.1366 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.6210 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 226ms/step - loss: 0.7611 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 822ms/step - loss: 0.8149 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 0.8112 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 371ms/step - loss: 0.8450 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 0.8616 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 530ms/step - loss: 0.4993 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 728ms/step - loss: 0.4980 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 418ms/step - loss: 0.6567 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 1.0763 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 934ms/step - loss: 0.6715 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.5029 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 456ms/step - loss: 0.4093 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 0.6305 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.8124 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 357ms/step - loss: 0.8266 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 0.5957 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.7214 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 463ms/step - loss: 0.5487 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.5789 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 539ms/step - loss: 0.8938 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 461ms/step - loss: 0.6890 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.3928 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.9947 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.4431 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8315 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.6254 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5135 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 0.7191 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.8426 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 0.4957 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 448ms/step - loss: 0.7737 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.7109 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 467ms/step - loss: 0.7901 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.8712 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 564ms/step - loss: 0.3958 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.8156 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 1.0812 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 0.3555 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.6872 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.7316 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 0.5108 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.6805 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.4973 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 353ms/step - loss: 1.0098 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.3655 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 0.7109 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 0.5585 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 0.5125 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 0.4101 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.7052 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.7738 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 595ms/step - loss: 0.5908 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.5674 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.7728 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 0.7926 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 125ms/step - loss: 0.9073 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 458ms/step - loss: 0.8278 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 1.1726 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 0.7491 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 0.8472 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 0.3651 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 482ms/step - loss: 0.7160 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.8131 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 0.6682 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 489ms/step - loss: 0.6215 - acc: 1.0000\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "val_loss for each sample at the end of epoch: [0.74833321571350098, 0.0]\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "val_loss for each sample at the end of epoch: [0.44291394948959351, 1.0]\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "val_loss for each sample at the end of epoch: [0.75165998935699463, 0.0]\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "val_loss for each sample at the end of epoch: [0.46361330151557922, 1.0]\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "val_loss for each sample at the end of epoch: [0.52998250722885132, 1.0]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "val_loss for each sample at the end of epoch: [0.81000334024429321, 0.0]\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "val_loss for each sample at the end of epoch: [0.81746089458465576, 0.0]\n",
      "1/1 [==============================] - 0s 23ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss for each sample at the end of epoch: [0.53582721948623657, 1.0]\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "val_loss for each sample at the end of epoch: [0.53804033994674683, 1.0]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "val_loss for each sample at the end of epoch: [0.74482285976409912, 0.0]\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "val_loss for each sample at the end of epoch: [0.84301853179931641, 0.0]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "val_loss for each sample at the end of epoch: [0.63661319017410278, 1.0]\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "val_loss for each sample at the end of epoch: [0.62095892429351807, 1.0]\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "val_loss for each sample at the end of epoch: [0.85991388559341431, 0.0]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "val_loss for each sample at the end of epoch: [0.66080641746520996, 1.0]\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "val_loss for each sample at the end of epoch: [0.45452791452407837, 1.0]\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "val_loss for each sample at the end of epoch: [0.93187016248703003, 0.0]\n",
      "1/1 [==============================] - 0s 198ms/step\n",
      "val_loss for each sample at the end of epoch: [0.91442549228668213, 0.0]\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "val_loss for each sample at the end of epoch: [0.45960396528244019, 1.0]\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "val_loss for each sample at the end of epoch: [0.6315879225730896, 1.0]\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "val_loss for each sample at the end of epoch: [0.51047229766845703, 1.0]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "val_loss for each sample at the end of epoch: [0.48932164907455444, 1.0]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "val_loss for each sample at the end of epoch: [0.46556025743484497, 1.0]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "val_loss for each sample at the end of epoch: [0.70031309127807617, 0.0]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "val_loss for each sample at the end of epoch: [0.74860799312591553, 0.0]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "val_loss for each sample at the end of epoch: [0.82915031909942627, 0.0]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "val_loss for each sample at the end of epoch: [0.55855584144592285, 1.0]\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "val_loss for each sample at the end of epoch: [0.75375360250473022, 0.0]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "val_loss for each sample at the end of epoch: [0.77697491645812988, 0.0]\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "val_loss for each sample at the end of epoch: [0.80519556999206543, 0.0]\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "val_loss for each sample at the end of epoch: [0.89903229475021362, 0.0]\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "val_loss for each sample at the end of epoch: [0.64837193489074707, 1.0]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "val_loss for each sample at the end of epoch: [1.056643009185791, 0.0]\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "val_loss for each sample at the end of epoch: [0.40701842308044434, 1.0]\n",
      "epoch: 2\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.7560 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 0.5655 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 532ms/step - loss: 0.6070 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 430ms/step - loss: 0.8268 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.8728 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 602ms/step - loss: 0.4444 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 563ms/step - loss: 1.0554 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 707ms/step - loss: 0.5375 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 0.6995 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 234ms/step - loss: 0.6205 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 765ms/step - loss: 0.4470 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 0.6091 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 554ms/step - loss: 0.6860 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8129 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 0.5047 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.7915 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 529ms/step - loss: 0.5994 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 0.5020 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 585ms/step - loss: 0.7703 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 0.7371 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7834 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 240ms/step - loss: 0.4788 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 159ms/step - loss: 1.0474 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.5942 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.5651 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 361ms/step - loss: 0.8288 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 0.6848 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 1.0805 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 388ms/step - loss: 0.3958 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 972ms/step - loss: 0.8639 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 0.6741 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 804ms/step - loss: 0.4188 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 405ms/step - loss: 0.8149 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.5392 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 0.9460 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 0.5871 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 0.4140 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 393ms/step - loss: 0.4952 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.9199 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.5467 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 408ms/step - loss: 0.4468 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 246ms/step - loss: 0.6916 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 517ms/step - loss: 0.5926 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.5594 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.5196 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 399ms/step - loss: 1.0644 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 551ms/step - loss: 0.8320 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 244ms/step - loss: 0.9298 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 250ms/step - loss: 0.4754 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 0.9455 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 535ms/step - loss: 0.7463 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.8800 - acc: 0.0000e+00\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 141ms/step - loss: 0.5632 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 608ms/step - loss: 0.4333 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.8448 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 547ms/step - loss: 0.7872 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 1.0264 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.4485 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 0.8372 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.8439 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.5756 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.5751 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 800ms/step - loss: 0.4987 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 329ms/step - loss: 0.5930 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 0.5328 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 448ms/step - loss: 0.8409 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.8280 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.7593 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 469ms/step - loss: 0.5604 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.6757 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.5033 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 537ms/step - loss: 0.9005 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 0.6909 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 0.7667 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.6046 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 633ms/step - loss: 0.4658 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.6414 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.5441 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.9715 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.6159 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 985ms/step - loss: 0.8089 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 423ms/step - loss: 0.4428 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 882ms/step - loss: 0.5005 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.9971 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 0.5542 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 0.4912 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 0.5299 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.5143 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.8785 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.4777 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.9143 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.9269 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.7706 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.9096 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.0278 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.9247 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.5640 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 604ms/step - loss: 0.7770 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.3602 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.6627 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.6443 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 564ms/step - loss: 0.5394 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.7183 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.4302 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 425ms/step - loss: 0.7196 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.2932 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 362ms/step - loss: 0.3371 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.4625 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.9130 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 0.6723 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.5509 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.7568 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 0.2741 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 1.2652 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 808ms/step - loss: 0.2703 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.7403 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 0.6702 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 529ms/step - loss: 0.5236 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 0.2702 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.7494 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.6362 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 0.7760 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 0.6910 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.7672 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 0.6999 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 1.1779 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.6093 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 0.7866 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 822ms/step - loss: 0.8384 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 0.7916 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 369ms/step - loss: 0.8620 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 0.8676 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 530ms/step - loss: 0.4678 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 723ms/step - loss: 0.4530 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 416ms/step - loss: 0.6452 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 1.2103 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 944ms/step - loss: 0.6819 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.5051 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 457ms/step - loss: 0.3822 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.6375 - acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 137ms/step - loss: 0.8100 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 354ms/step - loss: 0.8331 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 0.5937 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.7313 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 464ms/step - loss: 0.5325 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.5764 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 534ms/step - loss: 0.8864 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 483ms/step - loss: 0.6813 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.3335 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.0104 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.4293 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 995ms/step - loss: 0.8233 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.6266 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5076 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 0.6917 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.8075 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 0.4951 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 449ms/step - loss: 0.7480 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.7256 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 463ms/step - loss: 0.7912 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 0.8328 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 558ms/step - loss: 0.3986 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.8303 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 1.0468 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 0.3268 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 0.6620 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.7161 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 0.5052 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.6439 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.5106 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 355ms/step - loss: 1.0201 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.3197 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 336ms/step - loss: 0.6740 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.5595 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 0.5043 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 0.3666 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6726 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 0.7588 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 597ms/step - loss: 0.6019 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.5571 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.7254 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 0.7871 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.9457 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 461ms/step - loss: 0.8313 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 1.3805 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 240ms/step - loss: 0.7504 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 0.8441 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 166ms/step - loss: 0.2705 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 478ms/step - loss: 0.6955 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.8366 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 0.6725 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 488ms/step - loss: 0.6165 - acc: 1.0000\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "val_loss for each sample at the end of epoch: [0.72153711318969727, 0.0]\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "val_loss for each sample at the end of epoch: [0.38894081115722656, 1.0]\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "val_loss for each sample at the end of epoch: [0.73032283782958984, 0.0]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "val_loss for each sample at the end of epoch: [0.47047626972198486, 1.0]\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "val_loss for each sample at the end of epoch: [0.52364784479141235, 1.0]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "val_loss for each sample at the end of epoch: [0.8324282169342041, 0.0]\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "val_loss for each sample at the end of epoch: [0.80000728368759155, 0.0]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "val_loss for each sample at the end of epoch: [0.53381145000457764, 1.0]\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "val_loss for each sample at the end of epoch: [0.54098629951477051, 1.0]\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "val_loss for each sample at the end of epoch: [0.71994447708129883, 0.0]\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "val_loss for each sample at the end of epoch: [0.82829278707504272, 0.0]\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "val_loss for each sample at the end of epoch: [0.66380566358566284, 1.0]\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "val_loss for each sample at the end of epoch: [0.61328387260437012, 1.0]\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "val_loss for each sample at the end of epoch: [0.90493094921112061, 0.0]\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "val_loss for each sample at the end of epoch: [0.69475603103637695, 0.0]\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "val_loss for each sample at the end of epoch: [0.39937043190002441, 1.0]\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "val_loss for each sample at the end of epoch: [0.92613512277603149, 0.0]\n",
      "1/1 [==============================] - 0s 197ms/step\n",
      "val_loss for each sample at the end of epoch: [0.89324331283569336, 0.0]\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "val_loss for each sample at the end of epoch: [0.42401641607284546, 1.0]\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "val_loss for each sample at the end of epoch: [0.61122989654541016, 1.0]\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "val_loss for each sample at the end of epoch: [0.514548659324646, 1.0]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "val_loss for each sample at the end of epoch: [0.45560824871063232, 1.0]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "val_loss for each sample at the end of epoch: [0.4740869402885437, 1.0]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "val_loss for each sample at the end of epoch: [0.69457030296325684, 0.0]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "val_loss for each sample at the end of epoch: [0.75658071041107178, 0.0]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "val_loss for each sample at the end of epoch: [0.81744283437728882, 0.0]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "val_loss for each sample at the end of epoch: [0.54589354991912842, 1.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 112ms/step\n",
      "val_loss for each sample at the end of epoch: [0.75265371799468994, 0.0]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "val_loss for each sample at the end of epoch: [0.79938113689422607, 0.0]\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "val_loss for each sample at the end of epoch: [0.81919729709625244, 0.0]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "val_loss for each sample at the end of epoch: [0.92256307601928711, 0.0]\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "val_loss for each sample at the end of epoch: [0.62361705303192139, 1.0]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "val_loss for each sample at the end of epoch: [1.185791015625, 0.0]\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "val_loss for each sample at the end of epoch: [0.42347830533981323, 1.0]\n",
      "epoch: 3\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.7443 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 0.5663 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 527ms/step - loss: 0.6064 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 435ms/step - loss: 0.8470 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.8956 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 605ms/step - loss: 0.4507 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 565ms/step - loss: 1.0227 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 709ms/step - loss: 0.5160 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.6759 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 233ms/step - loss: 0.6352 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 769ms/step - loss: 0.4577 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 0.5814 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 554ms/step - loss: 0.6897 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8139 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 0.4925 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.7863 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 531ms/step - loss: 0.5685 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 0.4779 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 587ms/step - loss: 0.7725 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 0.7321 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7817 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 240ms/step - loss: 0.4852 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 1.0297 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.5864 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.5707 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 354ms/step - loss: 0.8524 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.6988 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 1.0628 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 394ms/step - loss: 0.4055 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 975ms/step - loss: 0.8435 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 0.6535 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 806ms/step - loss: 0.4267 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 398ms/step - loss: 0.8297 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.5161 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.9413 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 0.5915 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 0.4300 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 390ms/step - loss: 0.4872 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.9033 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.5339 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 410ms/step - loss: 0.4577 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 0.6509 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 518ms/step - loss: 0.5804 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 0.5474 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.5359 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 398ms/step - loss: 1.0584 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 555ms/step - loss: 0.8504 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 0.9397 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 0.4849 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 0.9390 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 531ms/step - loss: 0.7617 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 142ms/step - loss: 0.8773 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.5536 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 597ms/step - loss: 0.4379 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.8576 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 544ms/step - loss: 0.8121 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 156ms/step - loss: 1.0156 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.4504 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.8820 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.8334 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 134ms/step - loss: 0.5571 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 161ms/step - loss: 0.5499 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 828ms/step - loss: 0.4455 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 331ms/step - loss: 0.5627 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 226ms/step - loss: 0.5339 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 454ms/step - loss: 0.8739 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.8244 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.7249 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 473ms/step - loss: 0.5560 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.6700 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.5018 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 530ms/step - loss: 0.9038 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 0.6682 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 0.7692 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.5840 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 633ms/step - loss: 0.3984 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.6283 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.5407 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 1.0697 - acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6476 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 989ms/step - loss: 0.8135 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 428ms/step - loss: 0.3747 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 891ms/step - loss: 0.4554 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 0.9859 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 0.5398 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 0.4845 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 0.5148 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 0.5013 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.8266 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.4649 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.9277 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.9234 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.8046 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.9208 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 1.1108 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.9219 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.6068 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 608ms/step - loss: 0.8092 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.3133 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.6986 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.6267 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 562ms/step - loss: 0.5698 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.6901 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.4329 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 430ms/step - loss: 0.7445 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.2559 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 363ms/step - loss: 0.3388 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.4983 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.8319 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.6677 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.5888 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.7280 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 0.2560 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 1.2900 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 808ms/step - loss: 0.2724 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.7139 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 0.6675 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 525ms/step - loss: 0.5507 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 246ms/step - loss: 0.2623 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.7261 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 0.6230 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.7441 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 0.6831 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.7288 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 0.6745 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 1.1655 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.6103 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 0.7970 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 821ms/step - loss: 0.8446 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 0.7743 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 369ms/step - loss: 0.8651 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 0.8635 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 540ms/step - loss: 0.4639 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 721ms/step - loss: 0.4326 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 423ms/step - loss: 0.6367 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 1.2862 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 937ms/step - loss: 0.6875 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.5123 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 458ms/step - loss: 0.3787 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.6517 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.7948 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 359ms/step - loss: 0.8321 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 0.5940 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.7311 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 466ms/step - loss: 0.5307 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.5833 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 540ms/step - loss: 0.8738 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 462ms/step - loss: 0.6758 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.3032 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 1.0041 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.4302 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8140 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.6301 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.5089 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 0.6726 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.7784 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 384ms/step - loss: 0.4965 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 451ms/step - loss: 0.7324 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.7300 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 466ms/step - loss: 0.7921 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.7991 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 564ms/step - loss: 0.4107 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 0.8324 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 1.0055 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 0.3127 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 0.6491 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.7134 - acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 0.5052 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.6231 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.5292 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 355ms/step - loss: 1.0128 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.2931 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 0.6496 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.5615 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 0.5021 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 0.3471 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6521 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.7446 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 596ms/step - loss: 0.6125 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 0.5551 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.6850 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 0.7838 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 126ms/step - loss: 0.9640 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 461ms/step - loss: 0.8268 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 1.5098 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 242ms/step - loss: 0.7534 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 0.8355 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 165ms/step - loss: 0.2200 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 482ms/step - loss: 0.6740 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.8514 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 0.6707 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 489ms/step - loss: 0.6224 - acc: 1.0000\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "val_loss for each sample at the end of epoch: [0.69340670108795166, 0.0]\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "val_loss for each sample at the end of epoch: [0.35316777229309082, 1.0]\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "val_loss for each sample at the end of epoch: [0.71852445602416992, 0.0]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "val_loss for each sample at the end of epoch: [0.48095500469207764, 1.0]\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "val_loss for each sample at the end of epoch: [0.51820778846740723, 1.0]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "val_loss for each sample at the end of epoch: [0.85094970464706421, 0.0]\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "val_loss for each sample at the end of epoch: [0.77634400129318237, 0.0]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "val_loss for each sample at the end of epoch: [0.54083377122879028, 1.0]\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "val_loss for each sample at the end of epoch: [0.54749852418899536, 1.0]\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "val_loss for each sample at the end of epoch: [0.70379400253295898, 0.0]\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "val_loss for each sample at the end of epoch: [0.8156512975692749, 0.0]\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "val_loss for each sample at the end of epoch: [0.68430709838867188, 1.0]\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "val_loss for each sample at the end of epoch: [0.61415266990661621, 1.0]\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "val_loss for each sample at the end of epoch: [0.93730258941650391, 0.0]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "val_loss for each sample at the end of epoch: [0.72945678234100342, 0.0]\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "val_loss for each sample at the end of epoch: [0.36416852474212646, 1.0]\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "val_loss for each sample at the end of epoch: [0.9153742790222168, 0.0]\n",
      "1/1 [==============================] - 0s 204ms/step\n",
      "val_loss for each sample at the end of epoch: [0.87037301063537598, 0.0]\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "val_loss for each sample at the end of epoch: [0.39684033393859863, 1.0]\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "val_loss for each sample at the end of epoch: [0.59885978698730469, 1.0]\n",
      "1/1 [==============================] - 0s 6ms/step\n",
      "val_loss for each sample at the end of epoch: [0.51852560043334961, 1.0]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "val_loss for each sample at the end of epoch: [0.42531400918960571, 1.0]\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "val_loss for each sample at the end of epoch: [0.48719537258148193, 1.0]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "val_loss for each sample at the end of epoch: [0.69370126724243164, 0.0]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "val_loss for each sample at the end of epoch: [0.75268805027008057, 0.0]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "val_loss for each sample at the end of epoch: [0.80174237489700317, 0.0]\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "val_loss for each sample at the end of epoch: [0.54143881797790527, 1.0]\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "val_loss for each sample at the end of epoch: [0.7463454008102417, 0.0]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "val_loss for each sample at the end of epoch: [0.82070636749267578, 0.0]\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "val_loss for each sample at the end of epoch: [0.83537864685058594, 0.0]\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "val_loss for each sample at the end of epoch: [0.94017213582992554, 0.0]\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "val_loss for each sample at the end of epoch: [0.60672438144683838, 1.0]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "val_loss for each sample at the end of epoch: [1.2870912551879883, 0.0]\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "val_loss for each sample at the end of epoch: [0.44173935055732727, 1.0]\n",
      "epoch: 4\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.7256 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 0.5707 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 526ms/step - loss: 0.6149 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 434ms/step - loss: 0.8612 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.9153 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 601ms/step - loss: 0.4612 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 567ms/step - loss: 0.9910 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 707ms/step - loss: 0.4949 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 0.6670 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 234ms/step - loss: 0.6401 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 772ms/step - loss: 0.4700 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 0.5664 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 560ms/step - loss: 0.6791 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8058 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 0.4882 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.7926 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 533ms/step - loss: 0.5394 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 324ms/step - loss: 0.4613 - acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 585ms/step - loss: 0.7655 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 0.7353 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.7697 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 0.4951 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 1.0109 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 0.5838 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.5828 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 354ms/step - loss: 0.8644 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.7115 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 1.0449 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 392ms/step - loss: 0.4147 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 971ms/step - loss: 0.8165 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 0.6356 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 803ms/step - loss: 0.4346 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 400ms/step - loss: 0.8376 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.5000 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 0.9365 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 0.5989 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 0.4449 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 393ms/step - loss: 0.4815 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.8814 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.5260 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 405ms/step - loss: 0.4697 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 0.6143 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 519ms/step - loss: 0.5724 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 0.5418 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.5557 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 403ms/step - loss: 1.0492 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 567ms/step - loss: 0.8634 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 245ms/step - loss: 0.9422 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 244ms/step - loss: 0.4972 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 375ms/step - loss: 0.9263 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 529ms/step - loss: 0.7763 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 140ms/step - loss: 0.8694 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 141ms/step - loss: 0.5485 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 598ms/step - loss: 0.4406 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 0.8665 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 546ms/step - loss: 0.8319 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 155ms/step - loss: 1.0067 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 0.4514 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 0.9202 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.8290 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 136ms/step - loss: 0.5427 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.5314 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 802ms/step - loss: 0.4061 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 0.5375 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 0.5358 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 450ms/step - loss: 0.8999 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.8184 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.6938 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 472ms/step - loss: 0.5530 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.6648 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.5005 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 534ms/step - loss: 0.9025 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 0.6486 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 226ms/step - loss: 0.7667 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 123ms/step - loss: 0.5732 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 633ms/step - loss: 0.3552 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.6227 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 0.5412 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 1.1348 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6765 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 990ms/step - loss: 0.8110 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 425ms/step - loss: 0.3331 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 882ms/step - loss: 0.4341 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 0.9753 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 0.5307 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 0.4801 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 0.5112 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 0.4988 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.7775 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.4624 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.9364 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.9042 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.8270 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.9281 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.1271 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 0.9146 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.6447 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 608ms/step - loss: 0.8305 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.2925 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.7226 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.6195 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 565ms/step - loss: 0.5891 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.6746 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.4346 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 427ms/step - loss: 0.7571 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.2293 - acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 370ms/step - loss: 0.3407 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.5219 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.7778 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.6698 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.6086 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.7128 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 0.2435 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 1.2942 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 823ms/step - loss: 0.2699 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 0.7028 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 0.6708 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 521ms/step - loss: 0.5632 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 242ms/step - loss: 0.2519 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.7152 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 0.6155 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.7250 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 0.6827 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 0.7000 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 0.6591 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 1.1684 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 0.6135 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 226ms/step - loss: 0.8080 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 823ms/step - loss: 0.8492 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 0.7654 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 368ms/step - loss: 0.8693 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 0.8600 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 534ms/step - loss: 0.4591 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 906ms/step - loss: 0.4143 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 589ms/step - loss: 0.6272 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 1.3476 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6880 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 0.5162 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 642ms/step - loss: 0.3738 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.6627 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 0.7850 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 497ms/step - loss: 0.8325 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 457ms/step - loss: 0.5911 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.7304 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 656ms/step - loss: 0.5274 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.5883 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 755ms/step - loss: 0.8647 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 652ms/step - loss: 0.6703 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.2799 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 1.0018 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.4303 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8098 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 0.6322 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.5066 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 419ms/step - loss: 0.6559 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.7588 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 539ms/step - loss: 0.4929 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 627ms/step - loss: 0.7206 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.7324 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 649ms/step - loss: 0.7930 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 0.7726 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 788ms/step - loss: 0.4189 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 381ms/step - loss: 0.8362 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.9711 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 254ms/step - loss: 0.2982 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 404ms/step - loss: 0.6378 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 232ms/step - loss: 0.7139 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 402ms/step - loss: 0.5044 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.6065 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 0.5473 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 499ms/step - loss: 1.0059 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.2700 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 434ms/step - loss: 0.6281 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 0.5622 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 0.5004 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 371ms/step - loss: 0.3333 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.6322 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.7279 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 846ms/step - loss: 0.6273 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 0.5515 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.6477 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 0.7827 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 0.9754 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 672ms/step - loss: 0.8285 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 1.5998 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 339ms/step - loss: 0.7530 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 418ms/step - loss: 0.8331 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 232ms/step - loss: 0.1861 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 671ms/step - loss: 0.6586 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.8561 - acc: 0.0000e+00\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 0s 361ms/step - loss: 0.6732 - acc: 1.0000\n",
      "Epoch 1/1\n",
      "1/1 [==============================] - 1s 680ms/step - loss: 0.6259 - acc: 1.0000\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "val_loss for each sample at the end of epoch: [0.67019975185394287, 1.0]\n",
      "1/1 [==============================] - 0s 29ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss for each sample at the end of epoch: [0.32602107524871826, 1.0]\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "val_loss for each sample at the end of epoch: [0.70438587665557861, 0.0]\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "val_loss for each sample at the end of epoch: [0.48710477352142334, 1.0]\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "val_loss for each sample at the end of epoch: [0.51433223485946655, 1.0]\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "val_loss for each sample at the end of epoch: [0.86479818820953369, 0.0]\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "val_loss for each sample at the end of epoch: [0.75780266523361206, 0.0]\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "val_loss for each sample at the end of epoch: [0.54504668712615967, 1.0]\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "val_loss for each sample at the end of epoch: [0.54822862148284912, 1.0]\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "val_loss for each sample at the end of epoch: [0.68358743190765381, 1.0]\n",
      "1/1 [==============================] - 0s 131ms/step\n",
      "val_loss for each sample at the end of epoch: [0.80937552452087402, 0.0]\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "val_loss for each sample at the end of epoch: [0.71021956205368042, 0.0]\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "val_loss for each sample at the end of epoch: [0.61153852939605713, 1.0]\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "val_loss for each sample at the end of epoch: [0.96185344457626343, 0.0]\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "val_loss for each sample at the end of epoch: [0.75848484039306641, 0.0]\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "val_loss for each sample at the end of epoch: [0.33931949734687805, 1.0]\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "val_loss for each sample at the end of epoch: [0.91310977935791016, 0.0]\n",
      "1/1 [==============================] - 0s 294ms/step\n",
      "val_loss for each sample at the end of epoch: [0.8573182225227356, 0.0]\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "val_loss for each sample at the end of epoch: [0.37041670083999634, 1.0]\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "val_loss for each sample at the end of epoch: [0.5824589729309082, 1.0]\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "val_loss for each sample at the end of epoch: [0.52613043785095215, 1.0]\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "val_loss for each sample at the end of epoch: [0.39608108997344971, 1.0]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "val_loss for each sample at the end of epoch: [0.49637648463249207, 1.0]\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "val_loss for each sample at the end of epoch: [0.68894016742706299, 1.0]\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "val_loss for each sample at the end of epoch: [0.75164502859115601, 0.0]\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "val_loss for each sample at the end of epoch: [0.79288089275360107, 0.0]\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "val_loss for each sample at the end of epoch: [0.53294092416763306, 1.0]\n",
      "1/1 [==============================] - 0s 160ms/step\n",
      "val_loss for each sample at the end of epoch: [0.74667477607727051, 0.0]\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "val_loss for each sample at the end of epoch: [0.83880585432052612, 0.0]\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "val_loss for each sample at the end of epoch: [0.85271716117858887, 0.0]\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "val_loss for each sample at the end of epoch: [0.94949138164520264, 0.0]\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "val_loss for each sample at the end of epoch: [0.58564639091491699, 1.0]\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "val_loss for each sample at the end of epoch: [1.3745452165603638, 0.0]\n",
      "1/1 [==============================] - 0s 152ms/step\n",
      "val_loss for each sample at the end of epoch: [0.45627990365028381, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# Creating and fitting model\n",
    "model = create_LSTM_model(feature_size)\n",
    "epoch_histories = []\n",
    "for i in range(nb_epoch):\n",
    "    print(\"epoch: {}\".format(i))\n",
    "    train_history = []\n",
    "    for seq, label in zip(x_train, y_train):\n",
    "        loss = model.fit(np.array([seq]), label.reshape(1,2), epochs=1, batch_size=batch_size)\n",
    "        train_history.append(loss.history)\n",
    "    \n",
    "    val_history = []\n",
    "    for seq_val, label_val in zip(x_test, y_test):\n",
    "        val_loss = model.evaluate(np.array([seq_val]),label_val.reshape(1,2), batch_size=batch_size)\n",
    "        val_history.append(val_loss)\n",
    "        print(\"val_loss for each sample at the end of epoch: {}\".format(val_loss))\n",
    "    epoch_histories.append({\"train_history\": train_history, \"val_history\": val_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate_loss_acc():\n",
    "    for ep_hist in epoch_histories:\n",
    "        \n",
    "        tr_hist_list = ep_hist.get(\"train_history\")\n",
    "        train_acc = np.asscalar(np.mean(np.array([sample_history.get(\"acc\") for sample_history in tr_hist_list]), axis=0))\n",
    "        train_loss = np.asscalar(np.mean(np.array([sample_history.get(\"loss\") for sample_history in tr_hist_list]), axis=0))\n",
    "        \n",
    "        val_hist_list = ep_hist.get(\"val_history\")\n",
    "        \n",
    "        val_loss_acc = np.mean(np.array(val_hist_list), axis=0)\n",
    "        \n",
    "        yield np.hstack((np.array([train_loss, train_acc]), val_loss_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_loss_acc = np.array([s for s in accumulate_loss_acc()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3oAAAEyCAYAAACsx6JQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nz/w/zqlTgicZyDIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f724222fb38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (loss, acc) = plt.subplots(1,2)\n",
    "fig.set_size_inches(15,5)\n",
    "\n",
    "loss.plot(np.arange(nb_epoch), train_val_loss_acc[:,0], label=\"train_loss\")\n",
    "loss.plot(np.arange(nb_epoch), train_val_loss_acc[:,2], label=\"val_loss\")\n",
    "acc.plot(np.arange(nb_epoch), train_val_loss_acc[:,1], label=\"train_acc\")\n",
    "acc.plot(np.arange(nb_epoch), train_val_loss_acc[:,3], label=\"val_acc\")\n",
    "loss.legend();\n",
    "acc.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do prediction\n",
    "predictions = []\n",
    "for seq_test, label_test in zip(x_test, y_test):\n",
    "    pred = model.predict(np.array([seq_test]), batch_size=batch_size)\n",
    "    predictions.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.argmax(y_test, axis=1)\n",
    "y_pred = np.argmax(predictions, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f724222fa90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 32)                5504      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 5,570\n",
      "Trainable params: 5,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Test ROC Score: 0.522807\n",
      "Test RMSE Score: 0.685994\n",
      "Final Competition Score: 0.836813\n"
     ]
    }
   ],
   "source": [
    "# calculating scores\n",
    "roc_score = roc_auc_score(y_true, y_pred)\n",
    "# roc_score = roc_auc_score(np.argmax(y_test,axis=1), np.argmax(y_pred,axis=1))\n",
    "\n",
    "mse_score = mean_squared_error(y_true, y_pred)\n",
    "\n",
    "#printing and plotting model and score information\n",
    "# plot_loss(history)\n",
    "# plot_roc(y_test, y_pred)\n",
    "plot_roc(y_true, y_pred)\n",
    "# plot_accuracy(history)\n",
    "\n",
    "print(model.summary())\n",
    "print(\"Test ROC Score: %f\" % roc_score)\n",
    "print(\"Test RMSE Score: %f\" % sqrt(mse_score))\n",
    "print(\"Final Competition Score: %f\" % (1 - sqrt(mse_score) + roc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # do prediction\n",
    "# predictions = []\n",
    "# for seq_test, label_test in zip(x_test, y_test):\n",
    "#     pred = model.predict(np.array([seq_test]), batch_size=batch_size)\n",
    "#     predictions.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_competition = model.predict(x_competition_arr, batch_size=batch_size)\n",
    "\n",
    "# result_index = x_competition.reset_index(level=1, drop=True).index.unique()\n",
    "\n",
    "# argmax_preds = [np.argmax(predicted_label) for predicted_label in y_pred_competition]\n",
    "\n",
    "# result_df = DataFrame(argmax_preds, index=pd.Index(result_index, name='ITEST_id'), columns=['isSTEM'])\n",
    "\n",
    "# final_output = pd.concat([result_df, label_dataset.loc[shared_ids_with_train.values]]).sort_index()\n",
    "# final_output.to_csv(\"submition_1_{}.csv\".format(theNotebook))\n",
    "# final_output.isSTEM.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
